{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Documentation User Interface : Streamlit User Interface. Configuration : Configuration Details Core code - streamlit_interface If you found this helpful in your work, please cite: Godwin, R. C. & Melvin, R. L. Toward efficient data science: A comprehensive MLOps template for collaborative code development and automation. SoftwareX 101723 (2024) doi:10.1016/j.softx.2024.101723.","title":"Home"},{"location":"index.html#documentation","text":"User Interface : Streamlit User Interface. Configuration : Configuration Details Core code - streamlit_interface If you found this helpful in your work, please cite: Godwin, R. C. & Melvin, R. L. Toward efficient data science: A comprehensive MLOps template for collaborative code development and automation. SoftwareX 101723 (2024) doi:10.1016/j.softx.2024.101723.","title":"Documentation"},{"location":"UVTest/streamlit_interface.html","text":"BaseHandler A bare-bones helper that provides functionality for file upload and report generation. Source code in UVTest/streamlit_interface.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class BaseHandler : \"\"\" A bare-bones helper that provides functionality for file upload and report generation. \"\"\" def __init__ ( self , ui_helper = UIHelper ): # Expect ui_helper to be an object that provides Streamlit functionality (in this case, just st) self . ui = ui_helper self . config = Config def _ensure_file ( self , file , upload_message , file_types , key , info_message ): \"\"\" Checks if a file is provided; otherwise prompts the user to upload one. \"\"\" if file is None : file = self . ui . file_uploader ( label = upload_message , accept_multiple_files = False , type = file_types , key = key ) if file is None : self . ui . info ( info_message ) return file def _generate_dummy_report_download ( self ): \"\"\" Generates a dummy text report as bytes. \"\"\" report_text = \"Hello! \\n\\n This is your dummy report generated on demand.\" temp_stream = io . BytesIO () temp_stream . write ( report_text . encode ( \"utf-8\" )) temp_stream . seek ( 0 ) return temp_stream . read () def upload_example ( self ): \"\"\" Task A: Allows the user to upload a CSV file and then preview the first few rows. \"\"\" # Try to get a file; if none is provided, show an info message automatically file = self . _ensure_file ( file = None , upload_message = \"Please upload a CSV file\" , file_types = [ \"csv\" ], key = \"csv_uploader\" , info_message = \"You must upload a CSV file to proceed.\" , ) if file is not None : try : # Read and display the CSV contents df = pd . read_csv ( file ) # TODO Do something with the df... # example_task_a.process() self . ui . subheader ( \"CSV File Preview\" ) self . ui . dataframe ( df . head ()) self . ui . success ( \"CSV file loaded successfully!\" ) self . ui . balloons () except Exception as e : self . ui . error ( f \"Error reading CSV file: { e } \" ) def download_example ( self ): \"\"\" Task B: Generates a dummy report that the user can download. \"\"\" self . ui . subheader ( \"Generate a Dummy Report\" ) if self . ui . button ( \"Generate Report\" ): with self . ui . spinner ( \"Generating report...\" ): report_bytes = self . _generate_dummy_report_download () self . ui . download_button ( label = \"Download Dummy Report\" , data = report_bytes , file_name = \"dummy_report.txt\" , mime = \"text/plain\" , ) self . ui . success ( \"Report generated!\" ) self . ui . balloons () download_example () Task B: Generates a dummy report that the user can download. Source code in UVTest/streamlit_interface.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def download_example ( self ): \"\"\" Task B: Generates a dummy report that the user can download. \"\"\" self . ui . subheader ( \"Generate a Dummy Report\" ) if self . ui . button ( \"Generate Report\" ): with self . ui . spinner ( \"Generating report...\" ): report_bytes = self . _generate_dummy_report_download () self . ui . download_button ( label = \"Download Dummy Report\" , data = report_bytes , file_name = \"dummy_report.txt\" , mime = \"text/plain\" , ) self . ui . success ( \"Report generated!\" ) self . ui . balloons () upload_example () Task A: Allows the user to upload a CSV file and then preview the first few rows. Source code in UVTest/streamlit_interface.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def upload_example ( self ): \"\"\" Task A: Allows the user to upload a CSV file and then preview the first few rows. \"\"\" # Try to get a file; if none is provided, show an info message automatically file = self . _ensure_file ( file = None , upload_message = \"Please upload a CSV file\" , file_types = [ \"csv\" ], key = \"csv_uploader\" , info_message = \"You must upload a CSV file to proceed.\" , ) if file is not None : try : # Read and display the CSV contents df = pd . read_csv ( file ) # TODO Do something with the df... # example_task_a.process() self . ui . subheader ( \"CSV File Preview\" ) self . ui . dataframe ( df . head ()) self . ui . success ( \"CSV file loaded successfully!\" ) self . ui . balloons () except Exception as e : self . ui . error ( f \"Error reading CSV file: { e } \" )","title":"streamlit_interface"},{"location":"UVTest/streamlit_interface.html#UVTest.streamlit_interface.BaseHandler","text":"A bare-bones helper that provides functionality for file upload and report generation. Source code in UVTest/streamlit_interface.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class BaseHandler : \"\"\" A bare-bones helper that provides functionality for file upload and report generation. \"\"\" def __init__ ( self , ui_helper = UIHelper ): # Expect ui_helper to be an object that provides Streamlit functionality (in this case, just st) self . ui = ui_helper self . config = Config def _ensure_file ( self , file , upload_message , file_types , key , info_message ): \"\"\" Checks if a file is provided; otherwise prompts the user to upload one. \"\"\" if file is None : file = self . ui . file_uploader ( label = upload_message , accept_multiple_files = False , type = file_types , key = key ) if file is None : self . ui . info ( info_message ) return file def _generate_dummy_report_download ( self ): \"\"\" Generates a dummy text report as bytes. \"\"\" report_text = \"Hello! \\n\\n This is your dummy report generated on demand.\" temp_stream = io . BytesIO () temp_stream . write ( report_text . encode ( \"utf-8\" )) temp_stream . seek ( 0 ) return temp_stream . read () def upload_example ( self ): \"\"\" Task A: Allows the user to upload a CSV file and then preview the first few rows. \"\"\" # Try to get a file; if none is provided, show an info message automatically file = self . _ensure_file ( file = None , upload_message = \"Please upload a CSV file\" , file_types = [ \"csv\" ], key = \"csv_uploader\" , info_message = \"You must upload a CSV file to proceed.\" , ) if file is not None : try : # Read and display the CSV contents df = pd . read_csv ( file ) # TODO Do something with the df... # example_task_a.process() self . ui . subheader ( \"CSV File Preview\" ) self . ui . dataframe ( df . head ()) self . ui . success ( \"CSV file loaded successfully!\" ) self . ui . balloons () except Exception as e : self . ui . error ( f \"Error reading CSV file: { e } \" ) def download_example ( self ): \"\"\" Task B: Generates a dummy report that the user can download. \"\"\" self . ui . subheader ( \"Generate a Dummy Report\" ) if self . ui . button ( \"Generate Report\" ): with self . ui . spinner ( \"Generating report...\" ): report_bytes = self . _generate_dummy_report_download () self . ui . download_button ( label = \"Download Dummy Report\" , data = report_bytes , file_name = \"dummy_report.txt\" , mime = \"text/plain\" , ) self . ui . success ( \"Report generated!\" ) self . ui . balloons ()","title":"BaseHandler"},{"location":"UVTest/streamlit_interface.html#UVTest.streamlit_interface.BaseHandler.download_example","text":"Task B: Generates a dummy report that the user can download. Source code in UVTest/streamlit_interface.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def download_example ( self ): \"\"\" Task B: Generates a dummy report that the user can download. \"\"\" self . ui . subheader ( \"Generate a Dummy Report\" ) if self . ui . button ( \"Generate Report\" ): with self . ui . spinner ( \"Generating report...\" ): report_bytes = self . _generate_dummy_report_download () self . ui . download_button ( label = \"Download Dummy Report\" , data = report_bytes , file_name = \"dummy_report.txt\" , mime = \"text/plain\" , ) self . ui . success ( \"Report generated!\" ) self . ui . balloons ()","title":"download_example"},{"location":"UVTest/streamlit_interface.html#UVTest.streamlit_interface.BaseHandler.upload_example","text":"Task A: Allows the user to upload a CSV file and then preview the first few rows. Source code in UVTest/streamlit_interface.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def upload_example ( self ): \"\"\" Task A: Allows the user to upload a CSV file and then preview the first few rows. \"\"\" # Try to get a file; if none is provided, show an info message automatically file = self . _ensure_file ( file = None , upload_message = \"Please upload a CSV file\" , file_types = [ \"csv\" ], key = \"csv_uploader\" , info_message = \"You must upload a CSV file to proceed.\" , ) if file is not None : try : # Read and display the CSV contents df = pd . read_csv ( file ) # TODO Do something with the df... # example_task_a.process() self . ui . subheader ( \"CSV File Preview\" ) self . ui . dataframe ( df . head ()) self . ui . success ( \"CSV file loaded successfully!\" ) self . ui . balloons () except Exception as e : self . ui . error ( f \"Error reading CSV file: { e } \" )","title":"upload_example"},{"location":"UVTest/utils/utils_example.html","text":"task_a () This is the documentation for example task a Source code in UVTest/utils/utils_example.py 6 7 8 9 10 11 12 def task_a (): \"\"\" This is the documentation for example task a \"\"\" text_out = \"Demonstrating Streamlit behavior: You ran task a\" print ( text_out ) return text_out task_b () This is the documentation for example task b Source code in UVTest/utils/utils_example.py 15 16 17 18 19 20 21 def task_b (): \"\"\" This is the documentation for example task b \"\"\" text_out = \"Demonstrating Streamlit behavior: You ran task b\" print ( text_out ) return text_out","title":"utils_example"},{"location":"UVTest/utils/utils_example.html#UVTest.utils.utils_example.task_a","text":"This is the documentation for example task a Source code in UVTest/utils/utils_example.py 6 7 8 9 10 11 12 def task_a (): \"\"\" This is the documentation for example task a \"\"\" text_out = \"Demonstrating Streamlit behavior: You ran task a\" print ( text_out ) return text_out","title":"task_a"},{"location":"UVTest/utils/utils_example.html#UVTest.utils.utils_example.task_b","text":"This is the documentation for example task b Source code in UVTest/utils/utils_example.py 15 16 17 18 19 20 21 def task_b (): \"\"\" This is the documentation for example task b \"\"\" text_out = \"Demonstrating Streamlit behavior: You ran task b\" print ( text_out ) return text_out","title":"task_b"},{"location":"UVTest_config/config.html","text":"Config Directories prepopulated in the template Source code in UVTest_config/config.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class Config : # Development Directories \"\"\"Directories prepopulated in the template\"\"\" BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) # Data Directories DATA_DIR = Path ( \"/data/DATASCI\" ) RAW_DATA = Path ( DATA_DIR , \"raw\" ) INTERMEDIATE_DIR = Path ( DATA_DIR , \"intermediate\" ) RESULTS_DIR = Path ( DATA_DIR , \"results\" ) # Assets # Add assets here as needed. EXAMPLE_OUTPUT = Path ( INTERMEDIATE_DIR , \"Example_Output.csv\" ) # MLFlow model registry HEADER_MARKDOWN = \"\"\" # Update this\"\"\" # Logger logging_config = { \"version\" : 1 , \"disable_existing_loggers\" : False , \"formatters\" : { \"minimal\" : { \"format\" : \" %(message)s \" }, \"detailed\" : { \"format\" : \" %(levelname)s %(asctime)s [ %(name)s : %(filename)s : %(funcName)s : %(lineno)d ] \\n %(message)s \\n \" }, }, \"handlers\" : { \"console\" : { \"class\" : \"logging.StreamHandler\" , \"stream\" : sys . stdout , \"formatter\" : \"minimal\" , \"level\" : logging . DEBUG , }, \"info\" : { \"class\" : \"logging.handlers.RotatingFileHandler\" , \"filename\" : Path ( LOGS_DIR , \"info.log\" ), \"maxBytes\" : 10485760 , # 1 MB \"backupCount\" : 10 , \"formatter\" : \"detailed\" , \"level\" : logging . INFO , \"mode\" : \"a+\" , }, \"error\" : { \"class\" : \"logging.handlers.RotatingFileHandler\" , \"filename\" : Path ( LOGS_DIR , \"error.log\" ), \"maxBytes\" : 10485760 , # 1 MB \"backupCount\" : 10 , \"formatter\" : \"detailed\" , \"level\" : logging . ERROR , \"mode\" : \"a+\" , }, }, \"root\" : { \"handlers\" : [ \"console\" , \"info\" , \"error\" ], \"level\" : logging . INFO , \"propagate\" : True , }, } logging . config . dictConfig ( logging_config ) logger = logging . getLogger () logger . handlers [ 0 ] = RichHandler ( markup = True )","title":"Config"},{"location":"UVTest_config/config.html#UVTest_config.config.Config","text":"Directories prepopulated in the template Source code in UVTest_config/config.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class Config : # Development Directories \"\"\"Directories prepopulated in the template\"\"\" BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) # Data Directories DATA_DIR = Path ( \"/data/DATASCI\" ) RAW_DATA = Path ( DATA_DIR , \"raw\" ) INTERMEDIATE_DIR = Path ( DATA_DIR , \"intermediate\" ) RESULTS_DIR = Path ( DATA_DIR , \"results\" ) # Assets # Add assets here as needed. EXAMPLE_OUTPUT = Path ( INTERMEDIATE_DIR , \"Example_Output.csv\" ) # MLFlow model registry HEADER_MARKDOWN = \"\"\" # Update this\"\"\" # Logger logging_config = { \"version\" : 1 , \"disable_existing_loggers\" : False , \"formatters\" : { \"minimal\" : { \"format\" : \" %(message)s \" }, \"detailed\" : { \"format\" : \" %(levelname)s %(asctime)s [ %(name)s : %(filename)s : %(funcName)s : %(lineno)d ] \\n %(message)s \\n \" }, }, \"handlers\" : { \"console\" : { \"class\" : \"logging.StreamHandler\" , \"stream\" : sys . stdout , \"formatter\" : \"minimal\" , \"level\" : logging . DEBUG , }, \"info\" : { \"class\" : \"logging.handlers.RotatingFileHandler\" , \"filename\" : Path ( LOGS_DIR , \"info.log\" ), \"maxBytes\" : 10485760 , # 1 MB \"backupCount\" : 10 , \"formatter\" : \"detailed\" , \"level\" : logging . INFO , \"mode\" : \"a+\" , }, \"error\" : { \"class\" : \"logging.handlers.RotatingFileHandler\" , \"filename\" : Path ( LOGS_DIR , \"error.log\" ), \"maxBytes\" : 10485760 , # 1 MB \"backupCount\" : 10 , \"formatter\" : \"detailed\" , \"level\" : logging . ERROR , \"mode\" : \"a+\" , }, }, \"root\" : { \"handlers\" : [ \"console\" , \"info\" , \"error\" ], \"level\" : logging . INFO , \"propagate\" : True , }, } logging . config . dictConfig ( logging_config ) logger = logging . getLogger () logger . handlers [ 0 ] = RichHandler ( markup = True )","title":"Config"},{"location":"llm_utils/index.html","text":"Common code documentation AIWeb Common : Common code used throughout various projects of the UABPeriopAI team FastAPI helper apis schemas validators File Operations docx creator file config file handling text format upload manager Generate AugmentedResponse AugmentedServicer ChatResponse ChatSchemas ChatServicer PromptAssembler PromptyResponse PromptyServicer QueryInterface Response SingleResponse SingleResponseServicer Resourcing default resource config NIH RePorter Interface PubMed Interface PubMed Query Streamlit Bring Your Own Key (BYOK) Streamlit Common Object Factory Workflow Handler","title":"Home"},{"location":"llm_utils/index.html#common-code-documentation","text":"AIWeb Common : Common code used throughout various projects of the UABPeriopAI team FastAPI helper apis schemas validators File Operations docx creator file config file handling text format upload manager Generate AugmentedResponse AugmentedServicer ChatResponse ChatSchemas ChatServicer PromptAssembler PromptyResponse PromptyServicer QueryInterface Response SingleResponse SingleResponseServicer Resourcing default resource config NIH RePorter Interface PubMed Interface PubMed Query Streamlit Bring Your Own Key (BYOK) Streamlit Common Object Factory Workflow Handler","title":"Common code documentation"},{"location":"llm_utils/aiweb_common/ObjectFactory.html","text":"","title":"Object Factory"},{"location":"llm_utils/aiweb_common/WorkflowHandler.html","text":"WorkflowHandler Bases: ABC Source code in llm_utils/aiweb_common/WorkflowHandler.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 class WorkflowHandler ( ABC ): def __init__ ( self ): self . total_cost = 0.0 def _get_filename ( self ): # should not be forced. datafeasibility, for example, wouldn't use. raise NotImplementedError def _get_mime_type ( self ): # should not be forced. datafeasibility, for example, wouldn't use. raise NotImplementedError @abstractmethod def process ( self ): raise NotImplementedError def _update_total_cost ( self , response_meta ): self . total_cost += response_meta . total_cost def _get_db_connection ( self , db_server , db_name , db_user , db_password ): \"\"\" The function `get_db_connection` creates a database connection using the provided server, database name, user, and password. Args: db_server: The `db_server` parameter refers to the server where the database is hosted. This could be an IP address or a domain name pointing to the server where the SQL Server instance is running. db_name: The `db_name` parameter in the `get_db_connection` function refers to the name of the database you want to connect to on the specified database server. This parameter is used to construct the connection string that includes information such as the database name, server details, user credentials, and driver information for db_user: The `db_user` parameter in the `get_db_connection` function refers to the username used to authenticate and access the database. It is typically associated with a specific user account that has the necessary permissions to interact with the database specified by `db_name` on the server `db_server`. db_password: It seems like you were about to provide information about the `db_password` parameter but the text got cut off. Could you please provide the details or let me know how I can assist you further with the `db_password` parameter? Returns: The function `get_db_connection` returns a connection object to a SQL Server database using the provided server, database name, user, and password details. \"\"\" conn_str = ( \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=\" + db_server + \";DATABASE=\" + db_name + \";UID=\" + db_user + \";PWD=\" + db_password ) return pyodbc . connect ( conn_str ) def _write_to_db ( self , app_config , input_as_json , submit_time , response_time , cost , name_suffix = \"\" , ): \"\"\" The `write_to_db` function inserts data into a database table `api_interactions` with specified columns using the provided parameters. Args: app_config: The `app_config` parameter is a configuration object that contains information about the database server, database name, database user, and database password needed to establish a connection to the database. It likely contains attributes like `DB_SERVER`, `DB_NAME`, `DB_USER`, `DB_PASSWORD`, and `NAME input_as_json: The `input_as_json` parameter in the `write_to_db` function is expected to be a JSON object representing the user input data that you want to store in the database. This JSON object should be a serializable format that can be stored in a database column, typically a string representation of the submit_time: Submit time is the timestamp when the request was submitted to the API. It typically includes the date and time when the request was made. response_time: Response time is the time taken for the server to process a request and send a response back to the client. It is usually measured in milliseconds and indicates the efficiency of the system in handling requests. cost: The `cost` parameter in the `write_to_db` function represents the total cost associated with the API interaction being recorded in the database. This cost could be related to any expenses incurred during the interaction, such as processing fees, data storage costs, or any other relevant expenses. It is a numerical name_suffix: The `name_suffix` parameter in the `write_to_db` function is an optional parameter that allows you to append a suffix to the `app_name` field in the database. This can be useful if you need to differentiate between multiple instances of the same application in the database. If a `name \"\"\" with self . _get_db_connection ( db_server = app_config . DB_SERVER , db_name = app_config . DB_NAME , db_user = app_config . DB_USER , db_password = app_config . DB_PASSWORD , ) as conn : cursor = conn . cursor () query = \"\"\" INSERT INTO api_interactions (app_name, user_input, submit_time, response_time, total_cost) VALUES (?, ?, ?, ?, ?) \"\"\" cursor . execute ( query , ( app_config . NAME + name_suffix , input_as_json , submit_time , response_time , cost , ), ) conn . commit () def check_content_type ( self , returned_content ): # TODO: consider changing to if hasattr content if isinstance ( returned_content , AIMessage ): extracted_content = returned_content . content if isinstance ( returned_content , str ): extracted_content = returned_content else : raise TypeError ( \"Content not of type AIMessage or str. Check what invoke is returning. Langchain interfaces are inconsistent per API provider.\" ) return extracted_content # TODO is there a way to make this cleaner since self.promtpy_path and self._validate are only called in grandchildren def load_prompty ( self ): # self.prompty_path initialized by child # This should likely be broken up more with to isolate functionality further if not self . prompty_path . exists (): raise FileNotFoundError ( f \"Prompty file not found at: { self . prompty_path } \" ) with open ( self . prompty_path , \"r\" ) as f : prompty_content = f . read () prompty_data = list ( yaml . safe_load_all ( prompty_content )) if not prompty_data or len ( prompty_data ) < 2 : raise ValueError ( \"Invalid prompty file format.\" ) prompt_section = prompty_data [ 1 ] prompt_template = prompt_section . get ( \"prompt\" , {}) . get ( \"template\" , None ) if prompt_template is None : raise ValueError ( \"Prompt template not found in prompty file.\" ) self . _validate_prompt_template ( prompt_template ) return ChatPromptTemplate . from_template ( prompt_template , template_format = \"jinja2\" ) def log_to_database ( self , app_config , content_to_log , start , finish , background_tasks , label = \"\" ): \"\"\" This Python function logs content to a database using background tasks and handles KeyError exceptions. Args: app_config: The `app_config` parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The `content_to_log` parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The `start` parameter in the `log_to_database` method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the `log_to_database` method to log information to a database along with other parameters such as `app_config`, `content_to_log`, `start`, `background_tasks`, and an optional `label`. background_tasks: The `background_tasks` parameter in the `log_to_database` method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the `_write_to_db` method label: The `label` parameter in the `log_to_database` method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the `label` will be included \"\"\" try : background_tasks . add_task ( self . _write_to_db , app_config , content_to_log , start , finish , self . total_cost , label , ) except KeyError : raise KeyError ( \"Failed writing to database. Check interface configuration and try again.\" ) log_to_database ( app_config , content_to_log , start , finish , background_tasks , label = '' ) This Python function logs content to a database using background tasks and handles KeyError exceptions. Parameters: app_config \u2013 The app_config parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The content_to_log parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The start parameter in the log_to_database method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the log_to_database method to log information to a database along with other parameters such as app_config , content_to_log , start , background_tasks , and an optional label . background_tasks: The background_tasks parameter in the log_to_database method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the _write_to_db method label: The label parameter in the log_to_database method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the label will be included Source code in llm_utils/aiweb_common/WorkflowHandler.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def log_to_database ( self , app_config , content_to_log , start , finish , background_tasks , label = \"\" ): \"\"\" This Python function logs content to a database using background tasks and handles KeyError exceptions. Args: app_config: The `app_config` parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The `content_to_log` parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The `start` parameter in the `log_to_database` method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the `log_to_database` method to log information to a database along with other parameters such as `app_config`, `content_to_log`, `start`, `background_tasks`, and an optional `label`. background_tasks: The `background_tasks` parameter in the `log_to_database` method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the `_write_to_db` method label: The `label` parameter in the `log_to_database` method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the `label` will be included \"\"\" try : background_tasks . add_task ( self . _write_to_db , app_config , content_to_log , start , finish , self . total_cost , label , ) except KeyError : raise KeyError ( \"Failed writing to database. Check interface configuration and try again.\" ) manage_sensitive ( name ) The manage_sensitive function retrieves sensitive information from different sources based on deployment and development paths, as well as environment variables, and raises an error if the secret is not found. Parameters: name \u2013 The name parameter in the manage_sensitive function is used to specify the name of the sensitive information or secret that the function is trying to retrieve. The function first checks for the existence of the secret in a deployment path, then in a development path using glob, and finally as an environment Returns: \u2013 The manage_sensitive function is designed to manage sensitive information retrieval from different sources. It first checks for the secret file in the deployment path, then in the development path using glob, and finally as an environment variable. If the secret is found in any of these sources, it is returned. If no secret is found, a KeyError is raised with a message indicating that the secret with Source code in llm_utils/aiweb_common/WorkflowHandler.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def manage_sensitive ( name ): \"\"\" The `manage_sensitive` function retrieves sensitive information from different sources based on deployment and development paths, as well as environment variables, and raises an error if the secret is not found. Args: name: The `name` parameter in the `manage_sensitive` function is used to specify the name of the sensitive information or secret that the function is trying to retrieve. The function first checks for the existence of the secret in a deployment path, then in a development path using glob, and finally as an environment Returns: The `manage_sensitive` function is designed to manage sensitive information retrieval from different sources. It first checks for the secret file in the deployment path, then in the development path using glob, and finally as an environment variable. If the secret is found in any of these sources, it is returned. If no secret is found, a `KeyError` is raised with a message indicating that the secret with \"\"\" # Check in deployment path deploy_secret_fpath = f \"/run/secrets/ { name } \" if os . path . exists ( deploy_secret_fpath ): with open ( deploy_secret_fpath , \"r\" ) as file : return file . read () . rstrip ( \" \\n \" ) # Check in development path using glob develop_secret_paths = glob . glob ( f \"/workspaces/*/secrets/ { name } .txt\" ) if develop_secret_paths : # Assumes the first matching file is the correct one, adjust if necessary with open ( develop_secret_paths [ 0 ], \"r\" ) as file : return file . read () . rstrip ( \" \\n \" ) # Check environment variable last v1 = os . getenv ( name ) if v1 is not None : return v1 # If no secret is found raise KeyError ( f \"Secret { name } not found\" )","title":"Workflow Handler"},{"location":"llm_utils/aiweb_common/WorkflowHandler.html#aiweb_common.WorkflowHandler.WorkflowHandler","text":"Bases: ABC Source code in llm_utils/aiweb_common/WorkflowHandler.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 class WorkflowHandler ( ABC ): def __init__ ( self ): self . total_cost = 0.0 def _get_filename ( self ): # should not be forced. datafeasibility, for example, wouldn't use. raise NotImplementedError def _get_mime_type ( self ): # should not be forced. datafeasibility, for example, wouldn't use. raise NotImplementedError @abstractmethod def process ( self ): raise NotImplementedError def _update_total_cost ( self , response_meta ): self . total_cost += response_meta . total_cost def _get_db_connection ( self , db_server , db_name , db_user , db_password ): \"\"\" The function `get_db_connection` creates a database connection using the provided server, database name, user, and password. Args: db_server: The `db_server` parameter refers to the server where the database is hosted. This could be an IP address or a domain name pointing to the server where the SQL Server instance is running. db_name: The `db_name` parameter in the `get_db_connection` function refers to the name of the database you want to connect to on the specified database server. This parameter is used to construct the connection string that includes information such as the database name, server details, user credentials, and driver information for db_user: The `db_user` parameter in the `get_db_connection` function refers to the username used to authenticate and access the database. It is typically associated with a specific user account that has the necessary permissions to interact with the database specified by `db_name` on the server `db_server`. db_password: It seems like you were about to provide information about the `db_password` parameter but the text got cut off. Could you please provide the details or let me know how I can assist you further with the `db_password` parameter? Returns: The function `get_db_connection` returns a connection object to a SQL Server database using the provided server, database name, user, and password details. \"\"\" conn_str = ( \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=\" + db_server + \";DATABASE=\" + db_name + \";UID=\" + db_user + \";PWD=\" + db_password ) return pyodbc . connect ( conn_str ) def _write_to_db ( self , app_config , input_as_json , submit_time , response_time , cost , name_suffix = \"\" , ): \"\"\" The `write_to_db` function inserts data into a database table `api_interactions` with specified columns using the provided parameters. Args: app_config: The `app_config` parameter is a configuration object that contains information about the database server, database name, database user, and database password needed to establish a connection to the database. It likely contains attributes like `DB_SERVER`, `DB_NAME`, `DB_USER`, `DB_PASSWORD`, and `NAME input_as_json: The `input_as_json` parameter in the `write_to_db` function is expected to be a JSON object representing the user input data that you want to store in the database. This JSON object should be a serializable format that can be stored in a database column, typically a string representation of the submit_time: Submit time is the timestamp when the request was submitted to the API. It typically includes the date and time when the request was made. response_time: Response time is the time taken for the server to process a request and send a response back to the client. It is usually measured in milliseconds and indicates the efficiency of the system in handling requests. cost: The `cost` parameter in the `write_to_db` function represents the total cost associated with the API interaction being recorded in the database. This cost could be related to any expenses incurred during the interaction, such as processing fees, data storage costs, or any other relevant expenses. It is a numerical name_suffix: The `name_suffix` parameter in the `write_to_db` function is an optional parameter that allows you to append a suffix to the `app_name` field in the database. This can be useful if you need to differentiate between multiple instances of the same application in the database. If a `name \"\"\" with self . _get_db_connection ( db_server = app_config . DB_SERVER , db_name = app_config . DB_NAME , db_user = app_config . DB_USER , db_password = app_config . DB_PASSWORD , ) as conn : cursor = conn . cursor () query = \"\"\" INSERT INTO api_interactions (app_name, user_input, submit_time, response_time, total_cost) VALUES (?, ?, ?, ?, ?) \"\"\" cursor . execute ( query , ( app_config . NAME + name_suffix , input_as_json , submit_time , response_time , cost , ), ) conn . commit () def check_content_type ( self , returned_content ): # TODO: consider changing to if hasattr content if isinstance ( returned_content , AIMessage ): extracted_content = returned_content . content if isinstance ( returned_content , str ): extracted_content = returned_content else : raise TypeError ( \"Content not of type AIMessage or str. Check what invoke is returning. Langchain interfaces are inconsistent per API provider.\" ) return extracted_content # TODO is there a way to make this cleaner since self.promtpy_path and self._validate are only called in grandchildren def load_prompty ( self ): # self.prompty_path initialized by child # This should likely be broken up more with to isolate functionality further if not self . prompty_path . exists (): raise FileNotFoundError ( f \"Prompty file not found at: { self . prompty_path } \" ) with open ( self . prompty_path , \"r\" ) as f : prompty_content = f . read () prompty_data = list ( yaml . safe_load_all ( prompty_content )) if not prompty_data or len ( prompty_data ) < 2 : raise ValueError ( \"Invalid prompty file format.\" ) prompt_section = prompty_data [ 1 ] prompt_template = prompt_section . get ( \"prompt\" , {}) . get ( \"template\" , None ) if prompt_template is None : raise ValueError ( \"Prompt template not found in prompty file.\" ) self . _validate_prompt_template ( prompt_template ) return ChatPromptTemplate . from_template ( prompt_template , template_format = \"jinja2\" ) def log_to_database ( self , app_config , content_to_log , start , finish , background_tasks , label = \"\" ): \"\"\" This Python function logs content to a database using background tasks and handles KeyError exceptions. Args: app_config: The `app_config` parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The `content_to_log` parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The `start` parameter in the `log_to_database` method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the `log_to_database` method to log information to a database along with other parameters such as `app_config`, `content_to_log`, `start`, `background_tasks`, and an optional `label`. background_tasks: The `background_tasks` parameter in the `log_to_database` method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the `_write_to_db` method label: The `label` parameter in the `log_to_database` method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the `label` will be included \"\"\" try : background_tasks . add_task ( self . _write_to_db , app_config , content_to_log , start , finish , self . total_cost , label , ) except KeyError : raise KeyError ( \"Failed writing to database. Check interface configuration and try again.\" )","title":"WorkflowHandler"},{"location":"llm_utils/aiweb_common/WorkflowHandler.html#aiweb_common.WorkflowHandler.WorkflowHandler.log_to_database","text":"This Python function logs content to a database using background tasks and handles KeyError exceptions. Parameters: app_config \u2013 The app_config parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The content_to_log parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The start parameter in the log_to_database method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the log_to_database method to log information to a database along with other parameters such as app_config , content_to_log , start , background_tasks , and an optional label . background_tasks: The background_tasks parameter in the log_to_database method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the _write_to_db method label: The label parameter in the log_to_database method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the label will be included Source code in llm_utils/aiweb_common/WorkflowHandler.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def log_to_database ( self , app_config , content_to_log , start , finish , background_tasks , label = \"\" ): \"\"\" This Python function logs content to a database using background tasks and handles KeyError exceptions. Args: app_config: The `app_config` parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The `content_to_log` parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The `start` parameter in the `log_to_database` method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the `log_to_database` method to log information to a database along with other parameters such as `app_config`, `content_to_log`, `start`, `background_tasks`, and an optional `label`. background_tasks: The `background_tasks` parameter in the `log_to_database` method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the `_write_to_db` method label: The `label` parameter in the `log_to_database` method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the `label` will be included \"\"\" try : background_tasks . add_task ( self . _write_to_db , app_config , content_to_log , start , finish , self . total_cost , label , ) except KeyError : raise KeyError ( \"Failed writing to database. Check interface configuration and try again.\" )","title":"log_to_database"},{"location":"llm_utils/aiweb_common/WorkflowHandler.html#aiweb_common.WorkflowHandler.manage_sensitive","text":"The manage_sensitive function retrieves sensitive information from different sources based on deployment and development paths, as well as environment variables, and raises an error if the secret is not found. Parameters: name \u2013 The name parameter in the manage_sensitive function is used to specify the name of the sensitive information or secret that the function is trying to retrieve. The function first checks for the existence of the secret in a deployment path, then in a development path using glob, and finally as an environment Returns: \u2013 The manage_sensitive function is designed to manage sensitive information retrieval from different sources. It first checks for the secret file in the deployment path, then in the development path using glob, and finally as an environment variable. If the secret is found in any of these sources, it is returned. If no secret is found, a KeyError is raised with a message indicating that the secret with Source code in llm_utils/aiweb_common/WorkflowHandler.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def manage_sensitive ( name ): \"\"\" The `manage_sensitive` function retrieves sensitive information from different sources based on deployment and development paths, as well as environment variables, and raises an error if the secret is not found. Args: name: The `name` parameter in the `manage_sensitive` function is used to specify the name of the sensitive information or secret that the function is trying to retrieve. The function first checks for the existence of the secret in a deployment path, then in a development path using glob, and finally as an environment Returns: The `manage_sensitive` function is designed to manage sensitive information retrieval from different sources. It first checks for the secret file in the deployment path, then in the development path using glob, and finally as an environment variable. If the secret is found in any of these sources, it is returned. If no secret is found, a `KeyError` is raised with a message indicating that the secret with \"\"\" # Check in deployment path deploy_secret_fpath = f \"/run/secrets/ { name } \" if os . path . exists ( deploy_secret_fpath ): with open ( deploy_secret_fpath , \"r\" ) as file : return file . read () . rstrip ( \" \\n \" ) # Check in development path using glob develop_secret_paths = glob . glob ( f \"/workspaces/*/secrets/ { name } .txt\" ) if develop_secret_paths : # Assumes the first matching file is the correct one, adjust if necessary with open ( develop_secret_paths [ 0 ], \"r\" ) as file : return file . read () . rstrip ( \" \\n \" ) # Check environment variable last v1 = os . getenv ( name ) if v1 is not None : return v1 # If no secret is found raise KeyError ( f \"Secret { name } not found\" )","title":"manage_sensitive"},{"location":"llm_utils/aiweb_common/fastapi/helper_apis.html","text":"convert_file_to_base64 ( file = File ( ... )) async Internal API endpoint to convert files to base64-encoded strings. This endpoint is intended for use by API developers for testing. Source code in llm_utils/aiweb_common/fastapi/helper_apis.py 20 21 22 23 24 25 26 27 28 29 30 31 32 @router . post ( \"/internal/convert-to-base64/\" , include_in_schema = True ) async def convert_file_to_base64 ( file : UploadFile = File ( ... )): \"\"\" Internal API endpoint to convert files to base64-encoded strings. This endpoint is intended for use by API developers for testing. \"\"\" try : # Convert the uploaded file to base64 content = await file . read () encoded_string = base64 . b64encode ( content ) . decode ( \"utf-8\" ) return { \"base64\" : encoded_string } except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) decode_to_file ( request , background_tasks ) async Internal API endpoint to convert a base64-encoded string to a downloadable file. This endpoint is intended for use by API developers for testing. Source code in llm_utils/aiweb_common/fastapi/helper_apis.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @router . post ( \"/internal/decode-to-file/\" , include_in_schema = True ) async def decode_to_file ( request : DecodeRequest , background_tasks : BackgroundTasks ): \"\"\" Internal API endpoint to convert a base64-encoded string to a downloadable file. This endpoint is intended for use by API developers for testing. \"\"\" try : # Decode the base64 string file_bytes = base64 . b64decode ( request . encoded_data ) # Write the decoded bytes to a temporary file with tempfile . NamedTemporaryFile ( delete = False , suffix = f \". { request . file_extension } \" ) as tmp_file : tmp_file . write ( file_bytes ) tmp_file_path = tmp_file . name response = FileResponse ( tmp_file_path , filename = f \"decoded_file. { request . file_extension } \" , media_type = \"application/octet-stream\" , ) # Return a FileResponse that allows the file to be downloaded background_tasks . add_task ( os . unlink , tmp_file_path ) return response except Exception as e : raise HTTPException ( status_code = 500 , detail = f \"Failed to decode and generate file: { str ( e ) } \" )","title":"Helper APIS"},{"location":"llm_utils/aiweb_common/fastapi/helper_apis.html#aiweb_common.fastapi.helper_apis.convert_file_to_base64","text":"Internal API endpoint to convert files to base64-encoded strings. This endpoint is intended for use by API developers for testing. Source code in llm_utils/aiweb_common/fastapi/helper_apis.py 20 21 22 23 24 25 26 27 28 29 30 31 32 @router . post ( \"/internal/convert-to-base64/\" , include_in_schema = True ) async def convert_file_to_base64 ( file : UploadFile = File ( ... )): \"\"\" Internal API endpoint to convert files to base64-encoded strings. This endpoint is intended for use by API developers for testing. \"\"\" try : # Convert the uploaded file to base64 content = await file . read () encoded_string = base64 . b64encode ( content ) . decode ( \"utf-8\" ) return { \"base64\" : encoded_string } except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e ))","title":"convert_file_to_base64"},{"location":"llm_utils/aiweb_common/fastapi/helper_apis.html#aiweb_common.fastapi.helper_apis.decode_to_file","text":"Internal API endpoint to convert a base64-encoded string to a downloadable file. This endpoint is intended for use by API developers for testing. Source code in llm_utils/aiweb_common/fastapi/helper_apis.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @router . post ( \"/internal/decode-to-file/\" , include_in_schema = True ) async def decode_to_file ( request : DecodeRequest , background_tasks : BackgroundTasks ): \"\"\" Internal API endpoint to convert a base64-encoded string to a downloadable file. This endpoint is intended for use by API developers for testing. \"\"\" try : # Decode the base64 string file_bytes = base64 . b64decode ( request . encoded_data ) # Write the decoded bytes to a temporary file with tempfile . NamedTemporaryFile ( delete = False , suffix = f \". { request . file_extension } \" ) as tmp_file : tmp_file . write ( file_bytes ) tmp_file_path = tmp_file . name response = FileResponse ( tmp_file_path , filename = f \"decoded_file. { request . file_extension } \" , media_type = \"application/octet-stream\" , ) # Return a FileResponse that allows the file to be downloaded background_tasks . add_task ( os . unlink , tmp_file_path ) return response except Exception as e : raise HTTPException ( status_code = 500 , detail = f \"Failed to decode and generate file: { str ( e ) } \" )","title":"decode_to_file"},{"location":"llm_utils/aiweb_common/fastapi/schemas.html","text":"MSWordResponse Bases: BaseModel This class represents a response object containing a base64-encoded DOCX file that can be decoded to obtain the DOCX file. Source code in llm_utils/aiweb_common/fastapi/schemas.py 5 6 7 8 9 10 11 12 13 class MSWordResponse ( BaseModel ): \"\"\" This class represents a response object containing a base64-encoded DOCX file that can be decoded to obtain the DOCX file. \"\"\" encoded_docx : str = Field ( ... , description = \"Base64-encoded DOCX file. Decode to obtain the DOCX file.\" )","title":"Schemas"},{"location":"llm_utils/aiweb_common/fastapi/schemas.html#aiweb_common.fastapi.schemas.MSWordResponse","text":"Bases: BaseModel This class represents a response object containing a base64-encoded DOCX file that can be decoded to obtain the DOCX file. Source code in llm_utils/aiweb_common/fastapi/schemas.py 5 6 7 8 9 10 11 12 13 class MSWordResponse ( BaseModel ): \"\"\" This class represents a response object containing a base64-encoded DOCX file that can be decoded to obtain the DOCX file. \"\"\" encoded_docx : str = Field ( ... , description = \"Base64-encoded DOCX file. Decode to obtain the DOCX file.\" )","title":"MSWordResponse"},{"location":"llm_utils/aiweb_common/fastapi/validators.html","text":"","title":"Validators"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html","text":"DocxCreator Parent class that holds the common methods for creating DOCX reports, including the results summary and figures. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class DocxCreator : \"\"\" Parent class that holds the common methods for creating DOCX reports, including the results summary and figures. \"\"\" def __init__ ( self , results = None , figures = None ): self . results = results self . figures = figures def _add_table ( self , doc : Document , table_data : pd . DataFrame , heading : str , style : str = \"LightShading-Accent1\" ): \"\"\" Insert a table into the DOCX document. \"\"\" doc . add_heading ( heading , level = 2 ) table = doc . add_table ( rows = 1 , cols = len ( table_data . columns )) table . style = style # Header row. hdr_cells = table . rows [ 0 ] . cells for i , col_name in enumerate ( table_data . columns ): hdr_cells [ i ] . text = col_name # Populate the table with data. for _ , row in table_data . iterrows (): row_cells = table . add_row () . cells for i , cell_value in enumerate ( row ): row_cells [ i ] . text = str ( cell_value ) doc . add_paragraph () def _add_results_to_docx ( self , doc : Document , results : dict ): \"\"\" Insert results into the DOCX, including tables of metrics. \"\"\" doc . add_heading ( \"Results\" , level = 1 ) for method , metrics_data in results . items (): doc . add_heading ( method , level = 2 ) # Check if the data is already structured into separate sections if \"metrics\" in metrics_data : overall_metrics_df = metrics_data [ \"metrics\" ] else : # Create a dataframe from the flat metrics (excluding the classification report) # Here we assume that 'Classification Report' is separate; # Everything else is part of overall metrics. overall_metrics = { k : v for k , v in metrics_data . items () if k != \"Classification Report\" and not isinstance ( v , dict )} overall_metrics_df = pd . DataFrame ( list ( overall_metrics . items ()), columns = [ \"Metric\" , \"Value\" ]) self . _add_table ( doc , overall_metrics_df , \"Overall Metrics\" ) # For classification report, try to get it from either 'report' or 'Classification Report' if \"report\" in metrics_data : class_report_df = metrics_data [ \"report\" ] elif \"Classification Report\" in metrics_data : class_report_df = pd . DataFrame ( metrics_data [ \"Classification Report\" ]) . transpose () else : class_report_df = None if class_report_df is not None : self . _add_table ( doc , class_report_df , \"Classification Report\" ) # If there is any bootstrap information if \"bootstrap\" in metrics_data : self . _add_table ( doc , metrics_data [ \"bootstrap\" ], \"Bootstrap Confidence Intervals\" ) doc . add_paragraph () def create_docx_report ( self ) -> Document : doc = Document () doc . add_heading ( \"Model Evaluation Report\" , 0 ) # Add the method comparisons / metrics overview. if self . results : try : self . _add_results_to_docx ( doc , self . results ) except Exception as e : print ( \"Error in _add_results_to_docx:\" , e ) raise # Add figures / confusion matrices. if self . figures : doc . add_heading ( \"Figures\" , level = 1 ) for method , cm_fig in self . figures . items (): doc . add_heading ( method , level = 2 ) buf = io . BytesIO () try : cm_fig . savefig ( buf , format = \"png\" , bbox_inches = \"tight\" ) except Exception as e : print ( f \"Error saving figure for { method } : { e } \" ) raise buf . seek ( 0 ) try : doc . add_picture ( buf , width = Inches ( 5 )) except Exception as e : print ( f \"Error adding picture for { method } : { e } \" ) raise doc . add_paragraph () return doc FastAPIDocxCreator Bases: DocxCreator A specialized DocxCreator for FastAPI usage. Inherits the common DOCX-report functionality and adds a method to handle markdown-to-docx conversion. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class FastAPIDocxCreator ( DocxCreator ): \"\"\" A specialized DocxCreator for FastAPI usage. Inherits the common DOCX-report functionality and adds a method to handle markdown-to-docx conversion. \"\"\" def __init__ ( self , background_tasks : BackgroundTasks , results = None , figures = None ): super () . __init__ ( results = results , figures = figures ) self . background_tasks = background_tasks def convert_markdown_to_docx_bytes ( self , generated_response , template_filepath = None ) -> str : \"\"\" Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. \"\"\" print ( \"Converting markdown to temporary file\" ) temp_file_path = markdown_to_docx_temporary_file ( generated_response , template_location = template_filepath ) encoded_file = file_to_base64 ( temp_file_path ) # Convert file to base64 print ( \"File encoded\" ) self . background_tasks . add_task ( os . unlink , temp_file_path ) return encoded_file convert_markdown_to_docx_bytes ( generated_response , template_filepath = None ) Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 137 138 139 140 141 142 143 144 145 146 147 148 149 def convert_markdown_to_docx_bytes ( self , generated_response , template_filepath = None ) -> str : \"\"\" Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. \"\"\" print ( \"Converting markdown to temporary file\" ) temp_file_path = markdown_to_docx_temporary_file ( generated_response , template_location = template_filepath ) encoded_file = file_to_base64 ( temp_file_path ) # Convert file to base64 print ( \"File encoded\" ) self . background_tasks . add_task ( os . unlink , temp_file_path ) return encoded_file StreamlitDocxCreator Bases: DocxCreator A specialized DocxCreator for Streamlit usage. It inherits the _add_results_to_docx and create_docx_report methods directly from the parent class now, so there's no need to redefine them. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 113 114 115 116 117 118 119 120 121 class StreamlitDocxCreator ( DocxCreator ): \"\"\" A specialized DocxCreator for Streamlit usage. It inherits the _add_results_to_docx and create_docx_report methods directly from the parent class now, so there's no need to redefine them. \"\"\" def __init__ ( self , results , figures ): super () . __init__ ( results = results , figures = figures )","title":"Docx Creator"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html#aiweb_common.file_operations.docx_creator.DocxCreator","text":"Parent class that holds the common methods for creating DOCX reports, including the results summary and figures. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class DocxCreator : \"\"\" Parent class that holds the common methods for creating DOCX reports, including the results summary and figures. \"\"\" def __init__ ( self , results = None , figures = None ): self . results = results self . figures = figures def _add_table ( self , doc : Document , table_data : pd . DataFrame , heading : str , style : str = \"LightShading-Accent1\" ): \"\"\" Insert a table into the DOCX document. \"\"\" doc . add_heading ( heading , level = 2 ) table = doc . add_table ( rows = 1 , cols = len ( table_data . columns )) table . style = style # Header row. hdr_cells = table . rows [ 0 ] . cells for i , col_name in enumerate ( table_data . columns ): hdr_cells [ i ] . text = col_name # Populate the table with data. for _ , row in table_data . iterrows (): row_cells = table . add_row () . cells for i , cell_value in enumerate ( row ): row_cells [ i ] . text = str ( cell_value ) doc . add_paragraph () def _add_results_to_docx ( self , doc : Document , results : dict ): \"\"\" Insert results into the DOCX, including tables of metrics. \"\"\" doc . add_heading ( \"Results\" , level = 1 ) for method , metrics_data in results . items (): doc . add_heading ( method , level = 2 ) # Check if the data is already structured into separate sections if \"metrics\" in metrics_data : overall_metrics_df = metrics_data [ \"metrics\" ] else : # Create a dataframe from the flat metrics (excluding the classification report) # Here we assume that 'Classification Report' is separate; # Everything else is part of overall metrics. overall_metrics = { k : v for k , v in metrics_data . items () if k != \"Classification Report\" and not isinstance ( v , dict )} overall_metrics_df = pd . DataFrame ( list ( overall_metrics . items ()), columns = [ \"Metric\" , \"Value\" ]) self . _add_table ( doc , overall_metrics_df , \"Overall Metrics\" ) # For classification report, try to get it from either 'report' or 'Classification Report' if \"report\" in metrics_data : class_report_df = metrics_data [ \"report\" ] elif \"Classification Report\" in metrics_data : class_report_df = pd . DataFrame ( metrics_data [ \"Classification Report\" ]) . transpose () else : class_report_df = None if class_report_df is not None : self . _add_table ( doc , class_report_df , \"Classification Report\" ) # If there is any bootstrap information if \"bootstrap\" in metrics_data : self . _add_table ( doc , metrics_data [ \"bootstrap\" ], \"Bootstrap Confidence Intervals\" ) doc . add_paragraph () def create_docx_report ( self ) -> Document : doc = Document () doc . add_heading ( \"Model Evaluation Report\" , 0 ) # Add the method comparisons / metrics overview. if self . results : try : self . _add_results_to_docx ( doc , self . results ) except Exception as e : print ( \"Error in _add_results_to_docx:\" , e ) raise # Add figures / confusion matrices. if self . figures : doc . add_heading ( \"Figures\" , level = 1 ) for method , cm_fig in self . figures . items (): doc . add_heading ( method , level = 2 ) buf = io . BytesIO () try : cm_fig . savefig ( buf , format = \"png\" , bbox_inches = \"tight\" ) except Exception as e : print ( f \"Error saving figure for { method } : { e } \" ) raise buf . seek ( 0 ) try : doc . add_picture ( buf , width = Inches ( 5 )) except Exception as e : print ( f \"Error adding picture for { method } : { e } \" ) raise doc . add_paragraph () return doc","title":"DocxCreator"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html#aiweb_common.file_operations.docx_creator.FastAPIDocxCreator","text":"Bases: DocxCreator A specialized DocxCreator for FastAPI usage. Inherits the common DOCX-report functionality and adds a method to handle markdown-to-docx conversion. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class FastAPIDocxCreator ( DocxCreator ): \"\"\" A specialized DocxCreator for FastAPI usage. Inherits the common DOCX-report functionality and adds a method to handle markdown-to-docx conversion. \"\"\" def __init__ ( self , background_tasks : BackgroundTasks , results = None , figures = None ): super () . __init__ ( results = results , figures = figures ) self . background_tasks = background_tasks def convert_markdown_to_docx_bytes ( self , generated_response , template_filepath = None ) -> str : \"\"\" Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. \"\"\" print ( \"Converting markdown to temporary file\" ) temp_file_path = markdown_to_docx_temporary_file ( generated_response , template_location = template_filepath ) encoded_file = file_to_base64 ( temp_file_path ) # Convert file to base64 print ( \"File encoded\" ) self . background_tasks . add_task ( os . unlink , temp_file_path ) return encoded_file","title":"FastAPIDocxCreator"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html#aiweb_common.file_operations.docx_creator.FastAPIDocxCreator.convert_markdown_to_docx_bytes","text":"Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 137 138 139 140 141 142 143 144 145 146 147 148 149 def convert_markdown_to_docx_bytes ( self , generated_response , template_filepath = None ) -> str : \"\"\" Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. \"\"\" print ( \"Converting markdown to temporary file\" ) temp_file_path = markdown_to_docx_temporary_file ( generated_response , template_location = template_filepath ) encoded_file = file_to_base64 ( temp_file_path ) # Convert file to base64 print ( \"File encoded\" ) self . background_tasks . add_task ( os . unlink , temp_file_path ) return encoded_file","title":"convert_markdown_to_docx_bytes"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html#aiweb_common.file_operations.docx_creator.StreamlitDocxCreator","text":"Bases: DocxCreator A specialized DocxCreator for Streamlit usage. It inherits the _add_results_to_docx and create_docx_report methods directly from the parent class now, so there's no need to redefine them. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 113 114 115 116 117 118 119 120 121 class StreamlitDocxCreator ( DocxCreator ): \"\"\" A specialized DocxCreator for Streamlit usage. It inherits the _add_results_to_docx and create_docx_report methods directly from the parent class now, so there's no need to redefine them. \"\"\" def __init__ ( self , results , figures ): super () . __init__ ( results = results , figures = figures )","title":"StreamlitDocxCreator"},{"location":"llm_utils/aiweb_common/file_operations/file_config.html","text":"","title":"File Config"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html","text":"create_base64_file_validator ( * allowed_mime_types ) Creates a function that validates the MIME type of a base64-encoded file. Parameters: allowed_mime_types ( tuple , default: () ) \u2013 A tuple of strings representing the allowed MIME types. Returns: \u2013 Callable[[str, ValidationInfo], str]: A function to validate base64-encoded files. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def create_base64_file_validator ( * allowed_mime_types ): \"\"\" Creates a function that validates the MIME type of a base64-encoded file. Args: allowed_mime_types (tuple): A tuple of strings representing the allowed MIME types. Returns: Callable[[str, ValidationInfo], str]: A function to validate base64-encoded files. \"\"\" def validate_base64_encoded_file ( cls , v , info ): \"\"\" Validate the MIME type of a base64-encoded file. Raises ValueError if the MIME type is not what is expected. \"\"\" try : file_bytes = base64 . b64decode ( v , validate = True ) except ValueError : raise ValueError ( \"Invalid base64 encoding\" ) # Use python-magic to check MIME type mime = magic . Magic ( mime = True ) mime_type = mime . from_buffer ( file_bytes ) if mime_type not in allowed_mime_types : allowed_types_formatted = \", \" . join ( allowed_mime_types ) raise ValueError ( f \"Incorrect file type. Required types: { allowed_types_formatted } \" ) return v return validate_base64_encoded_file create_file_validator ( * allowed_mime_types ) Creates a dependency function that validates the MIME type of an uploaded file. Parameters: allowed_mime_types ( tuple , default: () ) \u2013 A tuple of strings representing the allowed MIME types. Returns: \u2013 Callable[[UploadFile], UploadFile]: A function that checks if the uploaded file's MIME type \u2013 is in the allowed MIME types. Examples: validate_docx_file = create_file_validator(\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\") validate_pdf_file = create_file_validator(\"application/pdf\", \"application/x-pdf\") Source code in llm_utils/aiweb_common/file_operations/file_handling.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def create_file_validator ( * allowed_mime_types ): \"\"\" Creates a dependency function that validates the MIME type of an uploaded file. Args: allowed_mime_types (tuple): A tuple of strings representing the allowed MIME types. Returns: Callable[[UploadFile], UploadFile]: A function that checks if the uploaded file's MIME type is in the allowed MIME types. Examples: validate_docx_file = create_file_validator(\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\") validate_pdf_file = create_file_validator(\"application/pdf\", \"application/x-pdf\") \"\"\" def validate_file ( file : UploadFile = File ( ... )): \"\"\" Validate the MIME type of the uploaded file. Raises HTTPException if the MIME type is not what is expected. \"\"\" if file . content_type not in allowed_mime_types : allowed_types_formatted = \", \" . join ( allowed_mime_types ) raise HTTPException ( status_code = 415 , detail = \"Incorrect file type. Required type: \" + allowed_types_formatted , ) return file return validate_file file_to_base64 ( filepath ) Converts a file to a base64-encoded string. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 12 13 14 15 def file_to_base64 ( filepath ): \"\"\"Converts a file to a base64-encoded string.\"\"\" with open ( filepath , \"rb\" ) as file : return base64 . b64encode ( file . read ()) . decode ( \"utf-8\" ) ingest_docx ( file ) async The function ingest_docx reads the content of a DOCX file as bytes, writes it to a temporary file, and then loads the document from the temporary file. :param file: The file parameter in the ingest_docx function seems to be a file-like object that supports asynchronous reading operations. When await file.read() is called, it reads the content of the file as bytes. This content is then written to a temporary file with a .docx :return: The function ingest_docx returns a tuple containing two values: 1. The name of the temporary file where the DOCX content was written. 2. An instance of the Document` class representing the loaded document from the temporary file. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 async def ingest_docx ( file ): \"\"\" The function `ingest_docx` reads the content of a DOCX file as bytes, writes it to a temporary file, and then loads the document from the temporary file. :param file: The `file` parameter in the `ingest_docx` function seems to be a file-like object that supports asynchronous reading operations. When `await file.read()` is called, it reads the content of the file as bytes. This content is then written to a temporary file with a `.docx :return: The function `ingest_docx` returns a tuple containing two values: 1. The name of the temporary file where the DOCX content was written. 2. An instance of the `Document` class representing the loaded document from the temporary file. \"\"\" with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_doc : content = await file . read () # Read file content as bytes # have to do this because of async context, otherwise calling function moves on temp_doc . write ( content ) temp_doc . flush () # Ensure all content is written to disk # Load the document from the temporary file return temp_doc . name , Document ( temp_doc . name ) # Load the document here ingest_docx_bytes ( content ) The function ingest_docx_bytes reads the content of a DOCX file from bytes, saves it to a temporary file, and then loads the document using the Document class. :param content: The ingest_docx_bytes function you provided seems to be designed to ingest the content of a DOCX file as bytes and load it using the python-docx library. However, it looks like the content parameter is missing in your message. Could you please provide the content parameter so :return: The ingest_docx_bytes function returns a tuple containing two values: 1. The name of the temporary file where the content was written (temp_doc.name) 2. The Document object representing the content loaded from the temporary file Source code in llm_utils/aiweb_common/file_operations/file_handling.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def ingest_docx_bytes ( content ): \"\"\" The function `ingest_docx_bytes` reads the content of a DOCX file from bytes, saves it to a temporary file, and then loads the document using the `Document` class. :param content: The `ingest_docx_bytes` function you provided seems to be designed to ingest the content of a DOCX file as bytes and load it using the `python-docx` library. However, it looks like the content parameter is missing in your message. Could you please provide the content parameter so :return: The `ingest_docx_bytes` function returns a tuple containing two values: 1. The name of the temporary file where the content was written (temp_doc.name) 2. The Document object representing the content loaded from the temporary file \"\"\" with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_doc : temp_doc . write ( content ) temp_doc . flush () # Ensure all content is written to disk # Load the document from the temporary file return temp_doc . name , Document ( temp_doc . name ) # Load the document here markdown_to_docx_temporary_file ( content , template_location = None ) The function prepare_docx_response converts Markdown content to a DOCX file and returns the temporary file path. :param content: The content parameter in the prepare_docx_response function is the text or data that you want to convert to a DOCX file. This content will be processed and converted into a DOCX file using the convert_markdown_docx function :param template_location: The prepare_docx_response function takes two parameters: :return: The function prepare_docx_response returns the file path of the temporary .docx file that is created after converting the provided content (in markdown format) to a .docx file using the specified template location. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def markdown_to_docx_temporary_file ( content , template_location = None ): \"\"\" The function `prepare_docx_response` converts Markdown content to a DOCX file and returns the temporary file path. :param content: The `content` parameter in the `prepare_docx_response` function is the text or data that you want to convert to a DOCX file. This content will be processed and converted into a DOCX file using the `convert_markdown_docx` function :param template_location: The `prepare_docx_response` function takes two parameters: :return: The function `prepare_docx_response` returns the file path of the temporary .docx file that is created after converting the provided content (in markdown format) to a .docx file using the specified template location. \"\"\" docx_data = convert_markdown_docx ( content , template_location ) # Using tempfile to save the output file temporarily with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_file : temp_file . write ( docx_data ) temp_file_path = temp_file . name return temp_file_path validate_date ( date_str = Query ( ... , description = 'The start date in YYYY-MM-DD format' )) Custom dependency that validates and parses a date string. Args: date_str (str): A date string in the YYYY-MM-DD format. Returns: datetime: The parsed datetime object. Raises: HTTPException: If the date string is not in the correct format. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def validate_date ( date_str : str = Query ( ... , description = \"The start date in YYYY-MM-DD format\" ) ) -> datetime : \"\"\" Custom dependency that validates and parses a date string. Args: date_str (str): A date string in the YYYY-MM-DD format. Returns: datetime: The parsed datetime object. Raises: HTTPException: If the date string is not in the correct format. \"\"\" try : return datetime . strptime ( date_str , \"%Y-%m- %d \" ) except ValueError as exc : raise HTTPException ( status_code = 400 , detail = \"start_date must be in YYYY-MM-DD format\" ) from exc","title":"File Handling"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.create_base64_file_validator","text":"Creates a function that validates the MIME type of a base64-encoded file. Parameters: allowed_mime_types ( tuple , default: () ) \u2013 A tuple of strings representing the allowed MIME types. Returns: \u2013 Callable[[str, ValidationInfo], str]: A function to validate base64-encoded files. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def create_base64_file_validator ( * allowed_mime_types ): \"\"\" Creates a function that validates the MIME type of a base64-encoded file. Args: allowed_mime_types (tuple): A tuple of strings representing the allowed MIME types. Returns: Callable[[str, ValidationInfo], str]: A function to validate base64-encoded files. \"\"\" def validate_base64_encoded_file ( cls , v , info ): \"\"\" Validate the MIME type of a base64-encoded file. Raises ValueError if the MIME type is not what is expected. \"\"\" try : file_bytes = base64 . b64decode ( v , validate = True ) except ValueError : raise ValueError ( \"Invalid base64 encoding\" ) # Use python-magic to check MIME type mime = magic . Magic ( mime = True ) mime_type = mime . from_buffer ( file_bytes ) if mime_type not in allowed_mime_types : allowed_types_formatted = \", \" . join ( allowed_mime_types ) raise ValueError ( f \"Incorrect file type. Required types: { allowed_types_formatted } \" ) return v return validate_base64_encoded_file","title":"create_base64_file_validator"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.create_file_validator","text":"Creates a dependency function that validates the MIME type of an uploaded file. Parameters: allowed_mime_types ( tuple , default: () ) \u2013 A tuple of strings representing the allowed MIME types. Returns: \u2013 Callable[[UploadFile], UploadFile]: A function that checks if the uploaded file's MIME type \u2013 is in the allowed MIME types. Examples: validate_docx_file = create_file_validator(\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\") validate_pdf_file = create_file_validator(\"application/pdf\", \"application/x-pdf\") Source code in llm_utils/aiweb_common/file_operations/file_handling.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def create_file_validator ( * allowed_mime_types ): \"\"\" Creates a dependency function that validates the MIME type of an uploaded file. Args: allowed_mime_types (tuple): A tuple of strings representing the allowed MIME types. Returns: Callable[[UploadFile], UploadFile]: A function that checks if the uploaded file's MIME type is in the allowed MIME types. Examples: validate_docx_file = create_file_validator(\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\") validate_pdf_file = create_file_validator(\"application/pdf\", \"application/x-pdf\") \"\"\" def validate_file ( file : UploadFile = File ( ... )): \"\"\" Validate the MIME type of the uploaded file. Raises HTTPException if the MIME type is not what is expected. \"\"\" if file . content_type not in allowed_mime_types : allowed_types_formatted = \", \" . join ( allowed_mime_types ) raise HTTPException ( status_code = 415 , detail = \"Incorrect file type. Required type: \" + allowed_types_formatted , ) return file return validate_file","title":"create_file_validator"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.file_to_base64","text":"Converts a file to a base64-encoded string. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 12 13 14 15 def file_to_base64 ( filepath ): \"\"\"Converts a file to a base64-encoded string.\"\"\" with open ( filepath , \"rb\" ) as file : return base64 . b64encode ( file . read ()) . decode ( \"utf-8\" )","title":"file_to_base64"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.ingest_docx","text":"The function ingest_docx reads the content of a DOCX file as bytes, writes it to a temporary file, and then loads the document from the temporary file. :param file: The file parameter in the ingest_docx function seems to be a file-like object that supports asynchronous reading operations. When await file.read() is called, it reads the content of the file as bytes. This content is then written to a temporary file with a .docx :return: The function ingest_docx returns a tuple containing two values: 1. The name of the temporary file where the DOCX content was written. 2. An instance of the Document` class representing the loaded document from the temporary file. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 async def ingest_docx ( file ): \"\"\" The function `ingest_docx` reads the content of a DOCX file as bytes, writes it to a temporary file, and then loads the document from the temporary file. :param file: The `file` parameter in the `ingest_docx` function seems to be a file-like object that supports asynchronous reading operations. When `await file.read()` is called, it reads the content of the file as bytes. This content is then written to a temporary file with a `.docx :return: The function `ingest_docx` returns a tuple containing two values: 1. The name of the temporary file where the DOCX content was written. 2. An instance of the `Document` class representing the loaded document from the temporary file. \"\"\" with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_doc : content = await file . read () # Read file content as bytes # have to do this because of async context, otherwise calling function moves on temp_doc . write ( content ) temp_doc . flush () # Ensure all content is written to disk # Load the document from the temporary file return temp_doc . name , Document ( temp_doc . name ) # Load the document here","title":"ingest_docx"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.ingest_docx_bytes","text":"The function ingest_docx_bytes reads the content of a DOCX file from bytes, saves it to a temporary file, and then loads the document using the Document class. :param content: The ingest_docx_bytes function you provided seems to be designed to ingest the content of a DOCX file as bytes and load it using the python-docx library. However, it looks like the content parameter is missing in your message. Could you please provide the content parameter so :return: The ingest_docx_bytes function returns a tuple containing two values: 1. The name of the temporary file where the content was written (temp_doc.name) 2. The Document object representing the content loaded from the temporary file Source code in llm_utils/aiweb_common/file_operations/file_handling.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def ingest_docx_bytes ( content ): \"\"\" The function `ingest_docx_bytes` reads the content of a DOCX file from bytes, saves it to a temporary file, and then loads the document using the `Document` class. :param content: The `ingest_docx_bytes` function you provided seems to be designed to ingest the content of a DOCX file as bytes and load it using the `python-docx` library. However, it looks like the content parameter is missing in your message. Could you please provide the content parameter so :return: The `ingest_docx_bytes` function returns a tuple containing two values: 1. The name of the temporary file where the content was written (temp_doc.name) 2. The Document object representing the content loaded from the temporary file \"\"\" with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_doc : temp_doc . write ( content ) temp_doc . flush () # Ensure all content is written to disk # Load the document from the temporary file return temp_doc . name , Document ( temp_doc . name ) # Load the document here","title":"ingest_docx_bytes"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.markdown_to_docx_temporary_file","text":"The function prepare_docx_response converts Markdown content to a DOCX file and returns the temporary file path. :param content: The content parameter in the prepare_docx_response function is the text or data that you want to convert to a DOCX file. This content will be processed and converted into a DOCX file using the convert_markdown_docx function :param template_location: The prepare_docx_response function takes two parameters: :return: The function prepare_docx_response returns the file path of the temporary .docx file that is created after converting the provided content (in markdown format) to a .docx file using the specified template location. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def markdown_to_docx_temporary_file ( content , template_location = None ): \"\"\" The function `prepare_docx_response` converts Markdown content to a DOCX file and returns the temporary file path. :param content: The `content` parameter in the `prepare_docx_response` function is the text or data that you want to convert to a DOCX file. This content will be processed and converted into a DOCX file using the `convert_markdown_docx` function :param template_location: The `prepare_docx_response` function takes two parameters: :return: The function `prepare_docx_response` returns the file path of the temporary .docx file that is created after converting the provided content (in markdown format) to a .docx file using the specified template location. \"\"\" docx_data = convert_markdown_docx ( content , template_location ) # Using tempfile to save the output file temporarily with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_file : temp_file . write ( docx_data ) temp_file_path = temp_file . name return temp_file_path","title":"markdown_to_docx_temporary_file"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.validate_date","text":"Custom dependency that validates and parses a date string. Args: date_str (str): A date string in the YYYY-MM-DD format. Returns: datetime: The parsed datetime object. Raises: HTTPException: If the date string is not in the correct format. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def validate_date ( date_str : str = Query ( ... , description = \"The start date in YYYY-MM-DD format\" ) ) -> datetime : \"\"\" Custom dependency that validates and parses a date string. Args: date_str (str): A date string in the YYYY-MM-DD format. Returns: datetime: The parsed datetime object. Raises: HTTPException: If the date string is not in the correct format. \"\"\" try : return datetime . strptime ( date_str , \"%Y-%m- %d \" ) except ValueError as exc : raise HTTPException ( status_code = 400 , detail = \"start_date must be in YYYY-MM-DD format\" ) from exc","title":"validate_date"},{"location":"llm_utils/aiweb_common/file_operations/text_format.html","text":"","title":"Text Format"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html","text":"BytesToDocx Bases: FastAPIUploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 class BytesToDocx ( FastAPIUploadManager ): def __init__ ( self , background_tasks ): super () . __init__ ( background_tasks ) def process_file_bytes ( self , file : bytes , extension = \".docx\" ) -> Document : \"\"\" The function `process_file_bytes` processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Args: file (bytes): The `file` parameter in the `process_file_bytes` method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a `.docx` file. If the provided extension is not `.docx`, a `TypeError` is raised. extension: The `extension` parameter in the `process_file_bytes` method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: The function `process_file_bytes` returns the content of a document (cv_in_docx) after processing a file given as bytes input. \"\"\" if extension != \".docx\" : raise TypeError cv_in_docx_filepath , cv_in_docx = ingest_docx_bytes ( file ) self . background_tasks . add_task ( os . unlink , cv_in_docx_filepath ) return cv_in_docx process_file_bytes ( file , extension = '.docx' ) The function process_file_bytes processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Parameters: file ( bytes ) \u2013 The file parameter in the process_file_bytes method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a .docx file. If the provided extension is not .docx , a TypeError is raised. extension: The extension parameter in the process_file_bytes method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: Document \u2013 The function process_file_bytes returns the content of a document (cv_in_docx) after processing a file given as bytes input. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def process_file_bytes ( self , file : bytes , extension = \".docx\" ) -> Document : \"\"\" The function `process_file_bytes` processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Args: file (bytes): The `file` parameter in the `process_file_bytes` method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a `.docx` file. If the provided extension is not `.docx`, a `TypeError` is raised. extension: The `extension` parameter in the `process_file_bytes` method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: The function `process_file_bytes` returns the content of a document (cv_in_docx) after processing a file given as bytes input. \"\"\" if extension != \".docx\" : raise TypeError cv_in_docx_filepath , cv_in_docx = ingest_docx_bytes ( file ) self . background_tasks . add_task ( os . unlink , cv_in_docx_filepath ) return cv_in_docx FastAPIUploadManager Bases: UploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 class FastAPIUploadManager ( UploadManager ): def __init__ ( self , background_tasks : BackgroundTasks ): self . background_tasks = background_tasks def process_file_bytes ( self , file : bytes , extension : str ) -> Union [ pd . DataFrame , str ]: \"\"\" Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Args: file_bytes (bytes): The byte-encoded content of the file. extension (str): The file extension indicating the file type. Returns: Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files or a markdown string for DOCX and TXT files. \"\"\" print ( \"Processing file with extension - \" , extension ) if extension == \".xlsx\" : print ( \"Opening Excel file\" ) return pd . read_excel ( BytesIO ( file )) elif extension == \".csv\" : return pd . read_csv ( BytesIO ( file )) elif extension == \".txt\" : print ( \"Reading text file\" ) return file . decode ( \"utf-8\" ) elif extension == \".docx\" : doc = Document ( BytesIO ( file )) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text elif extension == \".pdf\" : return self . read_pdf ( BytesIO ( file ), self . document_analysis_client ) else : print ( \"Converting file to Markdown\" ) with tempfile . NamedTemporaryFile ( delete = True , suffix = extension ) as tmpfile : tmpfile . write ( file ) tmpfile . seek ( 0 ) return pypandoc . convert_file ( tmpfile . name , \"markdown\" ) def read_and_validate_file ( self , encoded_file : str , extension : str ) -> Any : \"\"\" The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Args: encoded_file (str): The `encoded_file` parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the `read_file` method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: The `read_and_validate_file` method returns the output of the `read_file` method if it is not None. If the `read_file` method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. \"\"\" try : file_bytes = base64 . b64decode ( encoded_file ) output = self . process_file_bytes ( file_bytes , extension ) if output is None : raise HTTPException ( status_code = 422 , detail = \"Failed to process the file\" ) return output except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) from e process_file_bytes ( file , extension ) Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Parameters: file_bytes ( bytes ) \u2013 The byte-encoded content of the file. extension ( str ) \u2013 The file extension indicating the file type. Returns: Union [ DataFrame , str ] \u2013 Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files Union [ DataFrame , str ] \u2013 or a markdown string for DOCX and TXT files. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def process_file_bytes ( self , file : bytes , extension : str ) -> Union [ pd . DataFrame , str ]: \"\"\" Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Args: file_bytes (bytes): The byte-encoded content of the file. extension (str): The file extension indicating the file type. Returns: Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files or a markdown string for DOCX and TXT files. \"\"\" print ( \"Processing file with extension - \" , extension ) if extension == \".xlsx\" : print ( \"Opening Excel file\" ) return pd . read_excel ( BytesIO ( file )) elif extension == \".csv\" : return pd . read_csv ( BytesIO ( file )) elif extension == \".txt\" : print ( \"Reading text file\" ) return file . decode ( \"utf-8\" ) elif extension == \".docx\" : doc = Document ( BytesIO ( file )) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text elif extension == \".pdf\" : return self . read_pdf ( BytesIO ( file ), self . document_analysis_client ) else : print ( \"Converting file to Markdown\" ) with tempfile . NamedTemporaryFile ( delete = True , suffix = extension ) as tmpfile : tmpfile . write ( file ) tmpfile . seek ( 0 ) return pypandoc . convert_file ( tmpfile . name , \"markdown\" ) read_and_validate_file ( encoded_file , extension ) The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Parameters: encoded_file ( str ) \u2013 The encoded_file parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the read_file method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: Any \u2013 The read_and_validate_file method returns the output of the read_file method if it is not None. If the read_file method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def read_and_validate_file ( self , encoded_file : str , extension : str ) -> Any : \"\"\" The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Args: encoded_file (str): The `encoded_file` parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the `read_file` method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: The `read_and_validate_file` method returns the output of the `read_file` method if it is not None. If the `read_file` method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. \"\"\" try : file_bytes = base64 . b64decode ( encoded_file ) output = self . process_file_bytes ( file_bytes , extension ) if output is None : raise HTTPException ( status_code = 422 , detail = \"Failed to process the file\" ) return output except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) from e StreamlitUploadManager Bases: UploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class StreamlitUploadManager ( UploadManager ): def __init__ ( self , file = None , message : str = \"Please upload a file\" , file_types : list = None , accept_multiple_files : bool = False , document_analysis_client = None , ): \"\"\" Allows either an already-uploaded file (passed via `file`) or performs an interactive upload. Args: file: (Optional) an already-uploaded file object. message: The label for the uploader widget. file_types: List of allowed file extensions (default list if None). accept_multiple_files: Whether to allow multiple file uploads. document_analysis_client: (Optional) any additional client if needed. \"\"\" self . file = file self . message = message self . file_types = file_types if file_types is not None else [ \"csv\" , \"xlsx\" , \"docx\" , \"pdf\" , \"txt\" ] self . accept_multiple_files = accept_multiple_files self . document_analysis_client = document_analysis_client def process_upload ( self ): \"\"\" If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. \"\"\" # If no file was provided externally then invoke the uploader. if self . file is None : self . file = st . file_uploader ( label = self . message , type = self . file_types , accept_multiple_files = self . accept_multiple_files ) if not self . file : st . write ( \"Please upload a file to continue...\" ) return None , None # Call read_file on the provided file. extension = Path ( self . file . name ) . suffix return self . read_file ( self . file , extension = extension ) def upload_file ( self ): \"\"\" Wraps process_upload for backward compatibility. You can choose your naming. \"\"\" return self . process_upload () def read_file ( self , file , extension ): # print(\"Extension - \", extension) if extension == \".xlsx\" : return pd . read_excel ( file ), extension elif extension == \".docx\" : doc = Document ( file ) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text , extension elif extension == \".csv\" : return pd . read_csv ( file ), extension elif extension == \".pdf\" : return self . read_pdf ( file , self . document_analysis_client ), extension else : return None , None __init__ ( file = None , message = 'Please upload a file' , file_types = None , accept_multiple_files = False , document_analysis_client = None ) Allows either an already-uploaded file (passed via file ) or performs an interactive upload. Parameters: file \u2013 (Optional) an already-uploaded file object. message ( str , default: 'Please upload a file' ) \u2013 The label for the uploader widget. file_types ( list , default: None ) \u2013 List of allowed file extensions (default list if None). accept_multiple_files ( bool , default: False ) \u2013 Whether to allow multiple file uploads. document_analysis_client \u2013 (Optional) any additional client if needed. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , file = None , message : str = \"Please upload a file\" , file_types : list = None , accept_multiple_files : bool = False , document_analysis_client = None , ): \"\"\" Allows either an already-uploaded file (passed via `file`) or performs an interactive upload. Args: file: (Optional) an already-uploaded file object. message: The label for the uploader widget. file_types: List of allowed file extensions (default list if None). accept_multiple_files: Whether to allow multiple file uploads. document_analysis_client: (Optional) any additional client if needed. \"\"\" self . file = file self . message = message self . file_types = file_types if file_types is not None else [ \"csv\" , \"xlsx\" , \"docx\" , \"pdf\" , \"txt\" ] self . accept_multiple_files = accept_multiple_files self . document_analysis_client = document_analysis_client process_upload () If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def process_upload ( self ): \"\"\" If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. \"\"\" # If no file was provided externally then invoke the uploader. if self . file is None : self . file = st . file_uploader ( label = self . message , type = self . file_types , accept_multiple_files = self . accept_multiple_files ) if not self . file : st . write ( \"Please upload a file to continue...\" ) return None , None # Call read_file on the provided file. extension = Path ( self . file . name ) . suffix return self . read_file ( self . file , extension = extension ) upload_file () Wraps process_upload for backward compatibility. You can choose your naming. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 79 80 81 82 83 def upload_file ( self ): \"\"\" Wraps process_upload for backward compatibility. You can choose your naming. \"\"\" return self . process_upload ()","title":"Upload Manager"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.BytesToDocx","text":"Bases: FastAPIUploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 class BytesToDocx ( FastAPIUploadManager ): def __init__ ( self , background_tasks ): super () . __init__ ( background_tasks ) def process_file_bytes ( self , file : bytes , extension = \".docx\" ) -> Document : \"\"\" The function `process_file_bytes` processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Args: file (bytes): The `file` parameter in the `process_file_bytes` method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a `.docx` file. If the provided extension is not `.docx`, a `TypeError` is raised. extension: The `extension` parameter in the `process_file_bytes` method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: The function `process_file_bytes` returns the content of a document (cv_in_docx) after processing a file given as bytes input. \"\"\" if extension != \".docx\" : raise TypeError cv_in_docx_filepath , cv_in_docx = ingest_docx_bytes ( file ) self . background_tasks . add_task ( os . unlink , cv_in_docx_filepath ) return cv_in_docx","title":"BytesToDocx"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.BytesToDocx.process_file_bytes","text":"The function process_file_bytes processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Parameters: file ( bytes ) \u2013 The file parameter in the process_file_bytes method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a .docx file. If the provided extension is not .docx , a TypeError is raised. extension: The extension parameter in the process_file_bytes method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: Document \u2013 The function process_file_bytes returns the content of a document (cv_in_docx) after processing a file given as bytes input. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def process_file_bytes ( self , file : bytes , extension = \".docx\" ) -> Document : \"\"\" The function `process_file_bytes` processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Args: file (bytes): The `file` parameter in the `process_file_bytes` method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a `.docx` file. If the provided extension is not `.docx`, a `TypeError` is raised. extension: The `extension` parameter in the `process_file_bytes` method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: The function `process_file_bytes` returns the content of a document (cv_in_docx) after processing a file given as bytes input. \"\"\" if extension != \".docx\" : raise TypeError cv_in_docx_filepath , cv_in_docx = ingest_docx_bytes ( file ) self . background_tasks . add_task ( os . unlink , cv_in_docx_filepath ) return cv_in_docx","title":"process_file_bytes"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.FastAPIUploadManager","text":"Bases: UploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 class FastAPIUploadManager ( UploadManager ): def __init__ ( self , background_tasks : BackgroundTasks ): self . background_tasks = background_tasks def process_file_bytes ( self , file : bytes , extension : str ) -> Union [ pd . DataFrame , str ]: \"\"\" Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Args: file_bytes (bytes): The byte-encoded content of the file. extension (str): The file extension indicating the file type. Returns: Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files or a markdown string for DOCX and TXT files. \"\"\" print ( \"Processing file with extension - \" , extension ) if extension == \".xlsx\" : print ( \"Opening Excel file\" ) return pd . read_excel ( BytesIO ( file )) elif extension == \".csv\" : return pd . read_csv ( BytesIO ( file )) elif extension == \".txt\" : print ( \"Reading text file\" ) return file . decode ( \"utf-8\" ) elif extension == \".docx\" : doc = Document ( BytesIO ( file )) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text elif extension == \".pdf\" : return self . read_pdf ( BytesIO ( file ), self . document_analysis_client ) else : print ( \"Converting file to Markdown\" ) with tempfile . NamedTemporaryFile ( delete = True , suffix = extension ) as tmpfile : tmpfile . write ( file ) tmpfile . seek ( 0 ) return pypandoc . convert_file ( tmpfile . name , \"markdown\" ) def read_and_validate_file ( self , encoded_file : str , extension : str ) -> Any : \"\"\" The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Args: encoded_file (str): The `encoded_file` parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the `read_file` method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: The `read_and_validate_file` method returns the output of the `read_file` method if it is not None. If the `read_file` method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. \"\"\" try : file_bytes = base64 . b64decode ( encoded_file ) output = self . process_file_bytes ( file_bytes , extension ) if output is None : raise HTTPException ( status_code = 422 , detail = \"Failed to process the file\" ) return output except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) from e","title":"FastAPIUploadManager"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.FastAPIUploadManager.process_file_bytes","text":"Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Parameters: file_bytes ( bytes ) \u2013 The byte-encoded content of the file. extension ( str ) \u2013 The file extension indicating the file type. Returns: Union [ DataFrame , str ] \u2013 Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files Union [ DataFrame , str ] \u2013 or a markdown string for DOCX and TXT files. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def process_file_bytes ( self , file : bytes , extension : str ) -> Union [ pd . DataFrame , str ]: \"\"\" Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Args: file_bytes (bytes): The byte-encoded content of the file. extension (str): The file extension indicating the file type. Returns: Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files or a markdown string for DOCX and TXT files. \"\"\" print ( \"Processing file with extension - \" , extension ) if extension == \".xlsx\" : print ( \"Opening Excel file\" ) return pd . read_excel ( BytesIO ( file )) elif extension == \".csv\" : return pd . read_csv ( BytesIO ( file )) elif extension == \".txt\" : print ( \"Reading text file\" ) return file . decode ( \"utf-8\" ) elif extension == \".docx\" : doc = Document ( BytesIO ( file )) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text elif extension == \".pdf\" : return self . read_pdf ( BytesIO ( file ), self . document_analysis_client ) else : print ( \"Converting file to Markdown\" ) with tempfile . NamedTemporaryFile ( delete = True , suffix = extension ) as tmpfile : tmpfile . write ( file ) tmpfile . seek ( 0 ) return pypandoc . convert_file ( tmpfile . name , \"markdown\" )","title":"process_file_bytes"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.FastAPIUploadManager.read_and_validate_file","text":"The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Parameters: encoded_file ( str ) \u2013 The encoded_file parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the read_file method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: Any \u2013 The read_and_validate_file method returns the output of the read_file method if it is not None. If the read_file method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def read_and_validate_file ( self , encoded_file : str , extension : str ) -> Any : \"\"\" The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Args: encoded_file (str): The `encoded_file` parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the `read_file` method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: The `read_and_validate_file` method returns the output of the `read_file` method if it is not None. If the `read_file` method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. \"\"\" try : file_bytes = base64 . b64decode ( encoded_file ) output = self . process_file_bytes ( file_bytes , extension ) if output is None : raise HTTPException ( status_code = 422 , detail = \"Failed to process the file\" ) return output except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) from e","title":"read_and_validate_file"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.StreamlitUploadManager","text":"Bases: UploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class StreamlitUploadManager ( UploadManager ): def __init__ ( self , file = None , message : str = \"Please upload a file\" , file_types : list = None , accept_multiple_files : bool = False , document_analysis_client = None , ): \"\"\" Allows either an already-uploaded file (passed via `file`) or performs an interactive upload. Args: file: (Optional) an already-uploaded file object. message: The label for the uploader widget. file_types: List of allowed file extensions (default list if None). accept_multiple_files: Whether to allow multiple file uploads. document_analysis_client: (Optional) any additional client if needed. \"\"\" self . file = file self . message = message self . file_types = file_types if file_types is not None else [ \"csv\" , \"xlsx\" , \"docx\" , \"pdf\" , \"txt\" ] self . accept_multiple_files = accept_multiple_files self . document_analysis_client = document_analysis_client def process_upload ( self ): \"\"\" If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. \"\"\" # If no file was provided externally then invoke the uploader. if self . file is None : self . file = st . file_uploader ( label = self . message , type = self . file_types , accept_multiple_files = self . accept_multiple_files ) if not self . file : st . write ( \"Please upload a file to continue...\" ) return None , None # Call read_file on the provided file. extension = Path ( self . file . name ) . suffix return self . read_file ( self . file , extension = extension ) def upload_file ( self ): \"\"\" Wraps process_upload for backward compatibility. You can choose your naming. \"\"\" return self . process_upload () def read_file ( self , file , extension ): # print(\"Extension - \", extension) if extension == \".xlsx\" : return pd . read_excel ( file ), extension elif extension == \".docx\" : doc = Document ( file ) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text , extension elif extension == \".csv\" : return pd . read_csv ( file ), extension elif extension == \".pdf\" : return self . read_pdf ( file , self . document_analysis_client ), extension else : return None , None","title":"StreamlitUploadManager"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.StreamlitUploadManager.__init__","text":"Allows either an already-uploaded file (passed via file ) or performs an interactive upload. Parameters: file \u2013 (Optional) an already-uploaded file object. message ( str , default: 'Please upload a file' ) \u2013 The label for the uploader widget. file_types ( list , default: None ) \u2013 List of allowed file extensions (default list if None). accept_multiple_files ( bool , default: False ) \u2013 Whether to allow multiple file uploads. document_analysis_client \u2013 (Optional) any additional client if needed. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , file = None , message : str = \"Please upload a file\" , file_types : list = None , accept_multiple_files : bool = False , document_analysis_client = None , ): \"\"\" Allows either an already-uploaded file (passed via `file`) or performs an interactive upload. Args: file: (Optional) an already-uploaded file object. message: The label for the uploader widget. file_types: List of allowed file extensions (default list if None). accept_multiple_files: Whether to allow multiple file uploads. document_analysis_client: (Optional) any additional client if needed. \"\"\" self . file = file self . message = message self . file_types = file_types if file_types is not None else [ \"csv\" , \"xlsx\" , \"docx\" , \"pdf\" , \"txt\" ] self . accept_multiple_files = accept_multiple_files self . document_analysis_client = document_analysis_client","title":"__init__"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.StreamlitUploadManager.process_upload","text":"If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def process_upload ( self ): \"\"\" If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. \"\"\" # If no file was provided externally then invoke the uploader. if self . file is None : self . file = st . file_uploader ( label = self . message , type = self . file_types , accept_multiple_files = self . accept_multiple_files ) if not self . file : st . write ( \"Please upload a file to continue...\" ) return None , None # Call read_file on the provided file. extension = Path ( self . file . name ) . suffix return self . read_file ( self . file , extension = extension )","title":"process_upload"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.StreamlitUploadManager.upload_file","text":"Wraps process_upload for backward compatibility. You can choose your naming. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 79 80 81 82 83 def upload_file ( self ): \"\"\" Wraps process_upload for backward compatibility. You can choose your naming. \"\"\" return self . process_upload ()","title":"upload_file"},{"location":"llm_utils/aiweb_common/generate/AugmentedResponse.html","text":"","title":"AugmentedResponse"},{"location":"llm_utils/aiweb_common/generate/AugmentedServicer.html","text":"","title":"AugmentedServicer"},{"location":"llm_utils/aiweb_common/generate/ChatResponse.html","text":"","title":"ChatResponse"},{"location":"llm_utils/aiweb_common/generate/ChatSchemas.html","text":"","title":"ChatSchemas"},{"location":"llm_utils/aiweb_common/generate/ChatServicer.html","text":"","title":"ChatServicer"},{"location":"llm_utils/aiweb_common/generate/PromptAssembler.html","text":"","title":"PromptAssembler"},{"location":"llm_utils/aiweb_common/generate/PromptyResponseHandler.html","text":"","title":"PromptyResponse"},{"location":"llm_utils/aiweb_common/generate/PromptyServicer.html","text":"","title":"PromptyServicer"},{"location":"llm_utils/aiweb_common/generate/QueryInterface.html","text":"","title":"QueryInterface"},{"location":"llm_utils/aiweb_common/generate/Response.html","text":"","title":"Response"},{"location":"llm_utils/aiweb_common/generate/SingleResponse.html","text":"","title":"SingleResponse"},{"location":"llm_utils/aiweb_common/generate/SingleResponseServicer.html","text":"","title":"SingleResponseServicer"},{"location":"llm_utils/aiweb_common/resource/NIHRePORTERInterface.html","text":"","title":"NIH RePORTER Interface"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html","text":"PubMedInterface Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class PubMedInterface : def __init__ ( self , email = \"rmelvin@uabmc.edu\" , max_results = 50 , streamlit_context = False , max_retries = 3 , delay_seconds = 5 , ): self . email = email self . max_results = max_results self . streamlit_context = streamlit_context self . max_retries = max_retries self . delay_seconds = delay_seconds Entrez . email = email def _format_authors ( self ): \"\"\" The function `format_authors` takes a list of strings representing authors and returns a formatted string of their last names followed by initials, following APA rules. Args: author_list: A list of strings, where each string represents an author in the format \"Last Name Initials\". Returns: a formatted string of authors' names in the format \"Last Name, Initials,\" following APA rules. \"\"\" formatted_authors = [] num_authors = len ( self . _authors ) if num_authors <= 20 : # Normal case, just list all authors for author in self . _authors : * last_name , initials = author . rsplit ( \" \" , 1 ) last_name = \" \" . join ( last_name ) formatted_authors . append ( f \" { last_name } , { initials } .\" ) return \", \" . join ( formatted_authors ) else : # APA rule for > 20 authors: first 19, ellipsis, last author for author in self . _authors [: 19 ]: * last_name , initials = author . rsplit ( \" \" , 1 ) last_name = \" \" . join ( last_name ) formatted_authors . append ( f \" { last_name } , { initials } .\" ) last_author = self . _authors [ - 1 ] last_author_name , last_author_initials = last_author . rsplit ( \" \" , 1 ) formatted_authors . append ( \"\u2026\" ) formatted_authors . append ( f \" { last_author_name } , { last_author_initials } .\" ) return \", \" . join ( formatted_authors ) def _format_apa_citation ( self ): \"\"\" The function `format_apa_citation` takes in an article and its ID and returns a formatted APA citation string. Args: article: The `article` parameter is a dictionary that contains information about a specific article. It should have the following structure: article_id: The article_id parameter is the unique identifier for the article. It is used to include the PMID (PubMed ID) in the APA citation format. Returns: a formatted APA citation for an article, including the authors, publication year, title, journal, volume, pages, and PMID (PubMed ID). \"\"\" try : authors = self . _format_authors () except KeyError : authors = \"\" return f \" { authors } ( { self . _pub_month } ). { self . _title } { self . _journal } , { self . _volume } , { self . _pages } . PMID: { self . _pmid } \" def _extract_record_data ( self , record ): # Extract the desired information self . _title = record . get ( \"TI\" , \"No title available\" ) self . _keywords = record . get ( \"OT\" , []) # OT might not be present in all records # try to use mesh headers if keywords not present if not self . _keywords : self . _keywords = record . get ( \"MH\" , []) self . _abstract = record . get ( \"AB\" , \"No abstract available\" ) self . _pmid = record . get ( \"PMID\" , \"No PMID available\" ) self . _pub_month = record . get ( \"DP\" , \"No date available\" ) self . _authors = record . get ( \"AU\" , []) self . _journal = record . get ( \"JT\" , \"No jounral name available\" ) self . _volume = record . get ( \"VI\" , \"No volume available\" ) self . _pages = record . get ( \"PG\" , \"No pages available\" ) def search_pubmed_articles ( self , query ): \"\"\" The function `search_pubmed_articles` takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. \"\"\" for attempt in range ( self . max_retries ): try : handle = Entrez . esearch ( db = \"pubmed\" , term = query , sort = \"relevance\" , retmax = self . max_results ) record = Entrez . read ( handle ) handle . close () return record [ \"IdList\" ] except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( final_message ) return [] def fetch_article_details ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" pubmed_ids = [ str ( id ) for id in pubmed_ids ] ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , rettype = \"medline\" , retmode = \"text\" ) records = Medline . parse ( handle ) records = list ( records ) parsed_data = [] for record in records : self . _extract_record_data ( record ) citation = self . _format_apa_citation () parsed_data . append ( { \"date_published\" : self . _pub_month , \"title\" : self . _title , \"keywords\" : self . _keywords , \"abstract\" : self . _abstract , \"pmid\" : self . _pmid , \"authors\" : self . _authors , \"journal\" : self . _journal , \"citation\" : citation , } ) parsed_df = pd . DataFrame ( parsed_data ) return parsed_df except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) return None def fetch_article_details_xml ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , retmode = \"xml\" ) articles = Entrez . read ( handle )[ \"PubmedArticle\" ] handle . close () return articles except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : # TODO Ask about these imports... st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . error ( final_message ) return [] fetch_article_details ( pubmed_ids ) The function fetches article details from PubMed using the provided PubMed IDs. pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def fetch_article_details ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" pubmed_ids = [ str ( id ) for id in pubmed_ids ] ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , rettype = \"medline\" , retmode = \"text\" ) records = Medline . parse ( handle ) records = list ( records ) parsed_data = [] for record in records : self . _extract_record_data ( record ) citation = self . _format_apa_citation () parsed_data . append ( { \"date_published\" : self . _pub_month , \"title\" : self . _title , \"keywords\" : self . _keywords , \"abstract\" : self . _abstract , \"pmid\" : self . _pmid , \"authors\" : self . _authors , \"journal\" : self . _journal , \"citation\" : citation , } ) parsed_df = pd . DataFrame ( parsed_data ) return parsed_df except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) return None fetch_article_details_xml ( pubmed_ids ) The function fetches article details from PubMed using the provided PubMed IDs. pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def fetch_article_details_xml ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , retmode = \"xml\" ) articles = Entrez . read ( handle )[ \"PubmedArticle\" ] handle . close () return articles except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : # TODO Ask about these imports... st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . error ( final_message ) return [] search_pubmed_articles ( query ) The function search_pubmed_articles takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def search_pubmed_articles ( self , query ): \"\"\" The function `search_pubmed_articles` takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. \"\"\" for attempt in range ( self . max_retries ): try : handle = Entrez . esearch ( db = \"pubmed\" , term = query , sort = \"relevance\" , retmax = self . max_results ) record = Entrez . read ( handle ) handle . close () return record [ \"IdList\" ] except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( final_message ) return []","title":"PubMed Interface"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html#aiweb_common.resource.PubMedInterface.PubMedInterface","text":"Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class PubMedInterface : def __init__ ( self , email = \"rmelvin@uabmc.edu\" , max_results = 50 , streamlit_context = False , max_retries = 3 , delay_seconds = 5 , ): self . email = email self . max_results = max_results self . streamlit_context = streamlit_context self . max_retries = max_retries self . delay_seconds = delay_seconds Entrez . email = email def _format_authors ( self ): \"\"\" The function `format_authors` takes a list of strings representing authors and returns a formatted string of their last names followed by initials, following APA rules. Args: author_list: A list of strings, where each string represents an author in the format \"Last Name Initials\". Returns: a formatted string of authors' names in the format \"Last Name, Initials,\" following APA rules. \"\"\" formatted_authors = [] num_authors = len ( self . _authors ) if num_authors <= 20 : # Normal case, just list all authors for author in self . _authors : * last_name , initials = author . rsplit ( \" \" , 1 ) last_name = \" \" . join ( last_name ) formatted_authors . append ( f \" { last_name } , { initials } .\" ) return \", \" . join ( formatted_authors ) else : # APA rule for > 20 authors: first 19, ellipsis, last author for author in self . _authors [: 19 ]: * last_name , initials = author . rsplit ( \" \" , 1 ) last_name = \" \" . join ( last_name ) formatted_authors . append ( f \" { last_name } , { initials } .\" ) last_author = self . _authors [ - 1 ] last_author_name , last_author_initials = last_author . rsplit ( \" \" , 1 ) formatted_authors . append ( \"\u2026\" ) formatted_authors . append ( f \" { last_author_name } , { last_author_initials } .\" ) return \", \" . join ( formatted_authors ) def _format_apa_citation ( self ): \"\"\" The function `format_apa_citation` takes in an article and its ID and returns a formatted APA citation string. Args: article: The `article` parameter is a dictionary that contains information about a specific article. It should have the following structure: article_id: The article_id parameter is the unique identifier for the article. It is used to include the PMID (PubMed ID) in the APA citation format. Returns: a formatted APA citation for an article, including the authors, publication year, title, journal, volume, pages, and PMID (PubMed ID). \"\"\" try : authors = self . _format_authors () except KeyError : authors = \"\" return f \" { authors } ( { self . _pub_month } ). { self . _title } { self . _journal } , { self . _volume } , { self . _pages } . PMID: { self . _pmid } \" def _extract_record_data ( self , record ): # Extract the desired information self . _title = record . get ( \"TI\" , \"No title available\" ) self . _keywords = record . get ( \"OT\" , []) # OT might not be present in all records # try to use mesh headers if keywords not present if not self . _keywords : self . _keywords = record . get ( \"MH\" , []) self . _abstract = record . get ( \"AB\" , \"No abstract available\" ) self . _pmid = record . get ( \"PMID\" , \"No PMID available\" ) self . _pub_month = record . get ( \"DP\" , \"No date available\" ) self . _authors = record . get ( \"AU\" , []) self . _journal = record . get ( \"JT\" , \"No jounral name available\" ) self . _volume = record . get ( \"VI\" , \"No volume available\" ) self . _pages = record . get ( \"PG\" , \"No pages available\" ) def search_pubmed_articles ( self , query ): \"\"\" The function `search_pubmed_articles` takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. \"\"\" for attempt in range ( self . max_retries ): try : handle = Entrez . esearch ( db = \"pubmed\" , term = query , sort = \"relevance\" , retmax = self . max_results ) record = Entrez . read ( handle ) handle . close () return record [ \"IdList\" ] except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( final_message ) return [] def fetch_article_details ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" pubmed_ids = [ str ( id ) for id in pubmed_ids ] ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , rettype = \"medline\" , retmode = \"text\" ) records = Medline . parse ( handle ) records = list ( records ) parsed_data = [] for record in records : self . _extract_record_data ( record ) citation = self . _format_apa_citation () parsed_data . append ( { \"date_published\" : self . _pub_month , \"title\" : self . _title , \"keywords\" : self . _keywords , \"abstract\" : self . _abstract , \"pmid\" : self . _pmid , \"authors\" : self . _authors , \"journal\" : self . _journal , \"citation\" : citation , } ) parsed_df = pd . DataFrame ( parsed_data ) return parsed_df except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) return None def fetch_article_details_xml ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , retmode = \"xml\" ) articles = Entrez . read ( handle )[ \"PubmedArticle\" ] handle . close () return articles except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : # TODO Ask about these imports... st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . error ( final_message ) return []","title":"PubMedInterface"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html#aiweb_common.resource.PubMedInterface.PubMedInterface.fetch_article_details","text":"The function fetches article details from PubMed using the provided PubMed IDs. pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def fetch_article_details ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" pubmed_ids = [ str ( id ) for id in pubmed_ids ] ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , rettype = \"medline\" , retmode = \"text\" ) records = Medline . parse ( handle ) records = list ( records ) parsed_data = [] for record in records : self . _extract_record_data ( record ) citation = self . _format_apa_citation () parsed_data . append ( { \"date_published\" : self . _pub_month , \"title\" : self . _title , \"keywords\" : self . _keywords , \"abstract\" : self . _abstract , \"pmid\" : self . _pmid , \"authors\" : self . _authors , \"journal\" : self . _journal , \"citation\" : citation , } ) parsed_df = pd . DataFrame ( parsed_data ) return parsed_df except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) return None","title":"fetch_article_details"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html#aiweb_common.resource.PubMedInterface.PubMedInterface.fetch_article_details_xml","text":"The function fetches article details from PubMed using the provided PubMed IDs. pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def fetch_article_details_xml ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , retmode = \"xml\" ) articles = Entrez . read ( handle )[ \"PubmedArticle\" ] handle . close () return articles except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : # TODO Ask about these imports... st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . error ( final_message ) return []","title":"fetch_article_details_xml"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html#aiweb_common.resource.PubMedInterface.PubMedInterface.search_pubmed_articles","text":"The function search_pubmed_articles takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def search_pubmed_articles ( self , query ): \"\"\" The function `search_pubmed_articles` takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. \"\"\" for attempt in range ( self . max_retries ): try : handle = Entrez . esearch ( db = \"pubmed\" , term = query , sort = \"relevance\" , retmax = self . max_results ) record = Entrez . read ( handle ) handle . close () return record [ \"IdList\" ] except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( final_message ) return []","title":"search_pubmed_articles"},{"location":"llm_utils/aiweb_common/resource/PubMedQuery.html","text":"PubMedQueryGenerator Bases: WorkflowHandler Source code in llm_utils/aiweb_common/resource/PubMedQuery.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class PubMedQueryGenerator ( WorkflowHandler ): def __init__ ( self , LLM_INTERFACE , input_research_q , ): super () . __init__ () self . input_research_q = input_research_q self . single_response = SingleResponseHandler ( LLM_INTERFACE ) def generate_search_string ( self , loop_n = 0 , last_query = \"\" ): \"\"\" The function generates a search string for a research query using prompts and responses. Args: loop_n: The `loop_n` parameter in the `generate_search_string` method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The `last_query` parameter in the `generate_search_string` method is used to store the last query that was executed. It is then appended to the prompt if the `loop_n` parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: The function `generate_search_string` returns the response generated based on the assembled prompt, after updating the total cost. \"\"\" prompt = default_resource_config . PUBMED_QUERY_PROMPT . format ( self . input_research_q ) if loop_n > 0 : prompt = ( prompt + default_resource_config . PUBMED_FEW_RESULTS_PROMPT + last_query ) assembled_prompt = ( self . single_response . single_response_service . preparer . assemble_prompt ( system_prompt = default_resource_config . PUBMED_SYSTEM_PROMPT , user_prompt = prompt , ) ) response , response_meta = self . single_response . generate_response ( assembled_prompt ) self . _update_total_cost ( response_meta ) return response . content generate_search_string ( loop_n = 0 , last_query = '' ) The function generates a search string for a research query using prompts and responses. Parameters: loop_n \u2013 The loop_n parameter in the generate_search_string method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The last_query parameter in the generate_search_string method is used to store the last query that was executed. It is then appended to the prompt if the loop_n parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: \u2013 The function generate_search_string returns the response generated based on the assembled prompt, after updating the total cost. Source code in llm_utils/aiweb_common/resource/PubMedQuery.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def generate_search_string ( self , loop_n = 0 , last_query = \"\" ): \"\"\" The function generates a search string for a research query using prompts and responses. Args: loop_n: The `loop_n` parameter in the `generate_search_string` method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The `last_query` parameter in the `generate_search_string` method is used to store the last query that was executed. It is then appended to the prompt if the `loop_n` parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: The function `generate_search_string` returns the response generated based on the assembled prompt, after updating the total cost. \"\"\" prompt = default_resource_config . PUBMED_QUERY_PROMPT . format ( self . input_research_q ) if loop_n > 0 : prompt = ( prompt + default_resource_config . PUBMED_FEW_RESULTS_PROMPT + last_query ) assembled_prompt = ( self . single_response . single_response_service . preparer . assemble_prompt ( system_prompt = default_resource_config . PUBMED_SYSTEM_PROMPT , user_prompt = prompt , ) ) response , response_meta = self . single_response . generate_response ( assembled_prompt ) self . _update_total_cost ( response_meta ) return response . content","title":"PubMed Query"},{"location":"llm_utils/aiweb_common/resource/PubMedQuery.html#aiweb_common.resource.PubMedQuery.PubMedQueryGenerator","text":"Bases: WorkflowHandler Source code in llm_utils/aiweb_common/resource/PubMedQuery.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class PubMedQueryGenerator ( WorkflowHandler ): def __init__ ( self , LLM_INTERFACE , input_research_q , ): super () . __init__ () self . input_research_q = input_research_q self . single_response = SingleResponseHandler ( LLM_INTERFACE ) def generate_search_string ( self , loop_n = 0 , last_query = \"\" ): \"\"\" The function generates a search string for a research query using prompts and responses. Args: loop_n: The `loop_n` parameter in the `generate_search_string` method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The `last_query` parameter in the `generate_search_string` method is used to store the last query that was executed. It is then appended to the prompt if the `loop_n` parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: The function `generate_search_string` returns the response generated based on the assembled prompt, after updating the total cost. \"\"\" prompt = default_resource_config . PUBMED_QUERY_PROMPT . format ( self . input_research_q ) if loop_n > 0 : prompt = ( prompt + default_resource_config . PUBMED_FEW_RESULTS_PROMPT + last_query ) assembled_prompt = ( self . single_response . single_response_service . preparer . assemble_prompt ( system_prompt = default_resource_config . PUBMED_SYSTEM_PROMPT , user_prompt = prompt , ) ) response , response_meta = self . single_response . generate_response ( assembled_prompt ) self . _update_total_cost ( response_meta ) return response . content","title":"PubMedQueryGenerator"},{"location":"llm_utils/aiweb_common/resource/PubMedQuery.html#aiweb_common.resource.PubMedQuery.PubMedQueryGenerator.generate_search_string","text":"The function generates a search string for a research query using prompts and responses. Parameters: loop_n \u2013 The loop_n parameter in the generate_search_string method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The last_query parameter in the generate_search_string method is used to store the last query that was executed. It is then appended to the prompt if the loop_n parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: \u2013 The function generate_search_string returns the response generated based on the assembled prompt, after updating the total cost. Source code in llm_utils/aiweb_common/resource/PubMedQuery.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def generate_search_string ( self , loop_n = 0 , last_query = \"\" ): \"\"\" The function generates a search string for a research query using prompts and responses. Args: loop_n: The `loop_n` parameter in the `generate_search_string` method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The `last_query` parameter in the `generate_search_string` method is used to store the last query that was executed. It is then appended to the prompt if the `loop_n` parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: The function `generate_search_string` returns the response generated based on the assembled prompt, after updating the total cost. \"\"\" prompt = default_resource_config . PUBMED_QUERY_PROMPT . format ( self . input_research_q ) if loop_n > 0 : prompt = ( prompt + default_resource_config . PUBMED_FEW_RESULTS_PROMPT + last_query ) assembled_prompt = ( self . single_response . single_response_service . preparer . assemble_prompt ( system_prompt = default_resource_config . PUBMED_SYSTEM_PROMPT , user_prompt = prompt , ) ) response , response_meta = self . single_response . generate_response ( assembled_prompt ) self . _update_total_cost ( response_meta ) return response . content","title":"generate_search_string"},{"location":"llm_utils/aiweb_common/resource/default_resource_config.html","text":"","title":"Default Resource Config"},{"location":"llm_utils/aiweb_common/streamlit/BYOKLogin.html","text":"","title":"Bring Your Own Key (BYOK)"},{"location":"llm_utils/aiweb_common/streamlit/streamlit_common.html","text":"","title":"Streamlit Common"},{"location":"user_interface/UVTest_app.html","text":"The main app file sets up a Streamlit UI with tabs for two example tasks. Each tab calls a function from streamlit_handler to display its content. main () Main sets up the web app title, header and tabs. Source code in user_interface/UVTest_app.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def main (): \"\"\" Main sets up the web app title, header and tabs. \"\"\" st . set_page_config ( page_title = \"UVTest\" , page_icon = \"\ud83c\udff7\ufe0f\" ) st . title ( \"\ud83c\udff7\ufe0f UVTest \ud83e\udd16\" ) st . markdown ( Config . HEADER_MARKDOWN ) # Create a UI helper instance. (This can be used to wrap Streamlit calls if needed.) ui = UIHelper () handler = BaseHandler ( ui_helper = ui ) # Create two tabs in the app for the example tasks. tab1 , tab2 = st . tabs ([ \"Example Task A\" , \"Example Task B\" ]) with tab1 : # Call task A \u2014 CSV file upload and preview. ui . write ( task_a ()) handler . upload_example () with tab2 : # Call task B \u2014 Dummy report generation and download. ui . write ( task_b ()) handler . download_example ()","title":"User Interface"},{"location":"user_interface/UVTest_app.html#user_interface.UVTest_app.main","text":"Main sets up the web app title, header and tabs. Source code in user_interface/UVTest_app.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def main (): \"\"\" Main sets up the web app title, header and tabs. \"\"\" st . set_page_config ( page_title = \"UVTest\" , page_icon = \"\ud83c\udff7\ufe0f\" ) st . title ( \"\ud83c\udff7\ufe0f UVTest \ud83e\udd16\" ) st . markdown ( Config . HEADER_MARKDOWN ) # Create a UI helper instance. (This can be used to wrap Streamlit calls if needed.) ui = UIHelper () handler = BaseHandler ( ui_helper = ui ) # Create two tabs in the app for the example tasks. tab1 , tab2 = st . tabs ([ \"Example Task A\" , \"Example Task B\" ]) with tab1 : # Call task A \u2014 CSV file upload and preview. ui . write ( task_a ()) handler . upload_example () with tab2 : # Call task B \u2014 Dummy report generation and download. ui . write ( task_b ()) handler . download_example ()","title":"main"}]}